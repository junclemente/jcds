{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to jcds \ud83e\uddea","text":"<p>jcds is a personal collection of reusable Python functions for data science workflows, built to save time and reduce repetition in Jupyter notebooks.</p> <p>It provides: - \ud83d\udd0e Quick EDA tools (like <code>quick_report</code> and <code>long_report</code>) - \ud83d\udcca Helpers for working with categorical and continuous variables - \ud83e\uddf9 Utilities for inspecting, cleaning, and summarizing data - \u2601\ufe0f Optional AWS integration for working with S3 - \ud83e\uddea Well-tested functions using <code>pytest</code></p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#1-installation","title":"1. Installation","text":"<p>Install the latest version directly from GitHub:</p> <pre><code>pip install git+https://github.com/junclemente/jcds.git\n</code></pre> <p>Or with AWS extras: </p> <pre><code>pip install git+https://github.com/junclemente/jcds.git[aws]\n</code></pre>"},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>import pandas as pd\nimport jcds.eda as jeda \n\ndf = pd.read_csv(\"your_dataset.csv\")\nduplicates = jeda.show_dupes(df)\nprint(duplicates)\n\nunique_values_list = jeda.count_unique_values(df, ['col1', 'col2', 'col3'])\nprint(unique_values_list)\n\nunique_values_column = jeda.count_unique_values(df, 'col')\nprint(unique_values_column)\n\n</code></pre> <p>Check out the API Reference to explore all available functions.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#eda-inspect","title":"EDA Inspect","text":""},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na","title":"<code>count_cols_with_all_na(dataframe)</code>","text":"<p>Count the number of columns in the DataFrame where all values are missing (NaN).</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na--returns","title":"Returns","text":"<p>int     The number of columns where every row is NaN.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_cols_with_all_na(dataframe):\n    \"\"\"\n    Count the number of columns in the DataFrame where all values are missing (NaN).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of columns where every row is NaN.\n\n    \"\"\"\n\n    return dataframe.isna().all(axis=0).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na","title":"<code>count_cols_with_any_na(dataframe)</code>","text":"<p>Count the number of columns in the DataFrame that contain at least one missing (NaN) value.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na--returns","title":"Returns","text":"<p>int     The number of columns with at least one NaN value.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_cols_with_any_na(dataframe):\n    \"\"\"\n    Count the number of columns in the DataFrame that contain at least one missing (NaN) value.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of columns with at least one NaN value.\n\n    \"\"\"\n\n    return dataframe.isna().any(axis=0).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_id_like_columns","title":"<code>count_id_like_columns(dataframe, threshold=0.95)</code>","text":"<p>Count number of columns with high uniqueness (e.g., IDs).</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_id_like_columns(dataframe, threshold=0.95):\n    \"\"\"\n    Count number of columns with high uniqueness (e.g., IDs).\n    \"\"\"\n    total_rows = show_shape(dataframe)[0]\n    return sum(\n        dataframe[col].nunique() / total_rows &gt;= threshold for col in dataframe.columns\n    )\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na","title":"<code>count_rows_with_all_na(dataframe)</code>","text":"<p>Count the number of rows in the DataFrame where all values are missing (NaN).</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na--returns","title":"Returns","text":"<p>int     The number of rows where every column is NaN.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_rows_with_all_na(dataframe):\n    \"\"\"\n    Count the number of rows in the DataFrame where all values are missing (NaN).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of rows where every column is NaN.\n\n    \"\"\"\n\n    return dataframe.isna().all(axis=1).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na","title":"<code>count_rows_with_any_na(dataframe)</code>","text":"<p>Count the number of rows in the DataFrame that contain at least one missing (NaN) value.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na--returns","title":"Returns","text":"<p>int     The number of rows with at least one NaN value.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_rows_with_any_na(dataframe):\n    \"\"\"\n    Count the number of rows in the DataFrame that contain at least one missing (NaN) value.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of rows with at least one NaN value.\n\n    \"\"\"\n\n    return dataframe.isna().any(axis=1).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_total_na","title":"<code>count_total_na(dataframe)</code>","text":"<p>Calculate the total number of missing (NaN) values in the entire DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_total_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_total_na--returns","title":"Returns","text":"<p>int     The total count of NaN values in the DataFrame.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_total_na(dataframe):\n    \"\"\"\n    Calculate the total number of missing (NaN) values in the entire DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The total count of NaN values in the DataFrame.\n\n    \"\"\"\n\n    return dataframe.isna().sum().sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_unique_values","title":"<code>count_unique_values(dataframe, columns)</code>","text":"<p>Count the number of unique values in the specified columns of a DataFrame, including NaNs.</p>"},{"location":"api/#jcds.eda.inspect.count_unique_values--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. columns : list of str     A list of column names for which to count unique values.</p>"},{"location":"api/#jcds.eda.inspect.count_unique_values--returns","title":"Returns","text":"<p>dict     A dictionary where each key is a column name and the value is the count of unique entries, including NaNs.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def count_unique_values(dataframe, columns):\n    \"\"\"\n    Count the number of unique values in the specified columns of a DataFrame, including NaNs.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    columns : list of str\n        A list of column names for which to count unique values.\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a column name and the value is the count of unique entries, including NaNs.\n\n    \"\"\"\n\n    unique_counts = {}\n    for col in columns:\n        count = dataframe[col].nunique(dropna=False)\n        unique_counts[col] = count\n    return unique_counts\n</code></pre>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary","title":"<code>get_dtype_summary(dataframe)</code>","text":"<p>Returns a dictionary summarizing the count of common data types.</p>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary--returns","title":"Returns","text":"<p>dict     Keys are data types (as strings), values are column counts.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def get_dtype_summary(dataframe):\n    \"\"\"\n    Returns a dictionary summarizing the count of common data types.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n\n    Returns\n    -------\n    dict\n        Keys are data types (as strings), values are column counts.\n    \"\"\"\n    type_counts = {\n        \"object\": 0,\n        \"int\": 0,\n        \"float\": 0,\n        \"bool\": 0,\n        \"category\": 0,\n        \"datetime\": 0,\n        \"other\": 0,\n    }\n\n    for col in dataframe.columns:\n        dtype = dataframe[col].dtype\n\n        if ptypes.is_bool_dtype(dtype):\n            type_counts[\"bool\"] += 1\n        elif ptypes.is_integer_dtype(dtype):\n            type_counts[\"int\"] += 1\n        elif ptypes.is_float_dtype(dtype):\n            type_counts[\"float\"] += 1\n        elif isinstance(dtype, pd.CategoricalDtype):\n            type_counts[\"category\"] += 1\n        elif ptypes.is_datetime64_any_dtype(dtype):\n            type_counts[\"datetime\"] += 1\n        elif ptypes.is_object_dtype(dtype):\n            type_counts[\"object\"] += 1\n        else:\n            type_counts[\"other\"] += 1\n\n    return type_counts\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_binary_list","title":"<code>show_binary_list(dataframe, dropna=True)</code>","text":"<p>Identify binary columns in a DataFrame, optionally considering missing values.</p>"},{"location":"api/#jcds.eda.inspect.show_binary_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. dropna : bool, optional     If True, NaN values are excluded when determining binary columns. Default is True.</p>"},{"location":"api/#jcds.eda.inspect.show_binary_list--returns","title":"Returns","text":"<p>dict     A dictionary with two keys:         - \"binary_columns\": list of column names that contain exactly two unique non-null values.         - \"binary_with_nan\": list of column names that have two unique non-null values and also include NaNs.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_binary_list(dataframe, dropna=True):\n    \"\"\"\n    Identify binary columns in a DataFrame, optionally considering missing values.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    dropna : bool, optional\n        If True, NaN values are excluded when determining binary columns. Default is True.\n\n    Returns\n    -------\n    dict\n        A dictionary with two keys:\n            - \"binary_columns\": list of column names that contain exactly two unique non-null values.\n            - \"binary_with_nan\": list of column names that have two unique non-null values and also include NaNs.\n\n    \"\"\"\n\n    binary_cols = []\n    binary_with_nan = []\n    for col in dataframe.columns:\n        unique_vals = dataframe[col].unique()\n        unique_vals_no_nan = pd.Series(unique_vals).dropna().unique()\n\n        if len(unique_vals_no_nan) == 2:\n            if pd.isna(unique_vals).any():\n                binary_with_nan.append(col)\n            else:\n                binary_cols.append(col)\n\n    return {\"binary_columns\": binary_cols, \"binary_with_nan\": binary_with_nan}\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_catvar","title":"<code>show_catvar(dataframe)</code>","text":"<p>Identify and return a list of categorical variables in the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_catvar--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_catvar--returns","title":"Returns","text":"<p>list of str     A list of column names that have categorical or object data types.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_catvar(dataframe):\n    \"\"\"\n    Identify and return a list of categorical variables in the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names that have categorical or object data types.\n\n    \"\"\"\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    return cat_features\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_constantvars","title":"<code>show_constantvars(dataframe)</code>","text":"<p>Identify columns with only one unique value (including NaNs).</p>"},{"location":"api/#jcds.eda.inspect.show_constantvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame</p>"},{"location":"api/#jcds.eda.inspect.show_constantvars--returns","title":"Returns","text":"<p>list of str     Column names that are constant.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_constantvars(dataframe):\n    \"\"\"\n    Identify columns with only one unique value (including NaNs).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n\n    Returns\n    -------\n    list of str\n        Column names that are constant.\n    \"\"\"\n    print(\"Showing constant columns (only one unique value)\")\n    col_list = []\n    for col in dataframe.columns:\n        if dataframe[col].nunique(dropna=False) == 1:\n            col_list.append(col)\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_convar","title":"<code>show_convar(dataframe)</code>","text":"<p>Identify and return a list of continuous (non-categorical) variables in the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_convar--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_convar--returns","title":"Returns","text":"<p>list of str     A list of column names that are not of categorical or object data types.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_convar(dataframe):\n    \"\"\"\n    Identify and return a list of continuous (non-categorical) variables in the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names that are not of categorical or object data types.\n\n    \"\"\"\n\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    return cont_features\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns","title":"<code>show_datetime_columns(dataframe)</code>","text":"<p>Identify columns with datetime-like data types.</p>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns--returns","title":"Returns","text":"<p>list of str     A list of column names whose dtype is datetime64[ns] or similar.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_datetime_columns(dataframe):\n    \"\"\"\n    Identify columns with datetime-like data types.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names whose dtype is datetime64[ns] or similar.\n    \"\"\"\n    dt_cols = []\n    for col in dataframe.columns:\n        col_dtype = dataframe[col].dtype\n        if pd.api.types.is_datetime64_any_dtype(col_dtype):\n            dt_cols.append(col)\n    return dt_cols\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_dupes","title":"<code>show_dupes(dataframe)</code>","text":"<p>Return the number of duplicate rows in the given DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dupes--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dupes--returns","title":"Returns","text":"<p>int     The count of duplicated rows in the DataFrame.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_dupes(dataframe):\n    \"\"\"\n    Return the number of duplicate rows in the given DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The count of duplicated rows in the DataFrame.\n\n    \"\"\"\n    dupes = dataframe.duplicated()\n    return dupes.sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_highcardvars","title":"<code>show_highcardvars(dataframe, percent_unique=90)</code>","text":"<p>Identify categorical columns with high cardinality (&gt;= percent_unique).</p>"},{"location":"api/#jcds.eda.inspect.show_highcardvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. percent_unique : float, optional     Minimum % of unique values (vs. total rows) to consider high-cardinality.</p>"},{"location":"api/#jcds.eda.inspect.show_highcardvars--returns","title":"Returns","text":"<p>list of tuples     List of (column name, percent unique) tuples.</p> <p>Docstring generated with assistance from ChatGPT.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_highcardvars(dataframe, percent_unique=90):\n    \"\"\"\n    Identify categorical columns with high cardinality (&gt;= percent_unique).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    percent_unique : float, optional\n        Minimum % of unique values (vs. total rows) to consider high-cardinality.\n\n    Returns\n    -------\n    list of tuples\n        List of (column name, percent unique) tuples.\n\n    Docstring generated with assistance from ChatGPT.\n    \"\"\"\n    print(f\"Showing cat var of cardinality &gt;= {percent_unique}%\")\n    col_list = []\n    total_rows = show_shape(dataframe)[0]\n    cat_cols = show_catvar(dataframe)\n    for col in cat_cols:\n        count = dataframe[col].nunique()\n        percent = (count / total_rows) * 100\n        if percent &gt;= percent_unique:\n            col_list.append((col, percent))\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_lowcardvars","title":"<code>show_lowcardvars(dataframe, max_unique=10)</code>","text":"<p>Return a list of categorical variables with unique values less than or equal to the specified threshold.</p>"},{"location":"api/#jcds.eda.inspect.show_lowcardvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. max_unique : int, optional     The maximum number of unique values allowed for a variable to be considered low cardinality. Default is 10.</p>"},{"location":"api/#jcds.eda.inspect.show_lowcardvars--returns","title":"Returns","text":"<p>list of tuple     A list of tuples where each tuple contains the column name and the number of unique values.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_lowcardvars(dataframe, max_unique=10):\n    \"\"\"\n    Return a list of categorical variables with unique values less than or equal to the specified threshold.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    max_unique : int, optional\n        The maximum number of unique values allowed for a variable to be considered low cardinality. Default is 10.\n\n    Returns\n    -------\n    list of tuple\n        A list of tuples where each tuple contains the column name and the number of unique values.\n\n    \"\"\"\n\n    print(f\"Showing cat var of cardinality &lt;= {max_unique}\")\n    col_list = []\n    cols = show_catvar(dataframe)\n    for col in cols:\n        count = dataframe[col].nunique()\n        if count &lt;= max_unique:\n            col_list.append((col, count))\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_memory_use","title":"<code>show_memory_use(dataframe)</code>","text":"<p>Returns memory usage of the dataframe in megabytes (MB)</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_memory_use(dataframe):\n    \"\"\"\n    Returns memory usage of the dataframe in megabytes (MB)\n    \"\"\"\n    memory_usage = dataframe.memory_usage(deep=True).sum()\n    # convert from bytes to megabytes\n    memory_usage = memory_usage / 1024 ** 2\n    return float(memory_usage)\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns","title":"<code>show_possible_datetime_columns(dataframe, sample_size=5)</code>","text":"<p>Identify object columns that may contain datetime-like strings.</p>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. sample_size : int     Number of values to sample for date parsing test.</p>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns--returns","title":"Returns","text":"<p>list of str     Columns that appear to contain datetime-like strings.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_possible_datetime_columns(dataframe, sample_size=5):\n    \"\"\"\n    Identify object columns that may contain datetime-like strings.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    sample_size : int\n        Number of values to sample for date parsing test.\n\n    Returns\n    -------\n    list of str\n        Columns that appear to contain datetime-like strings.\n    \"\"\"\n    possible_date_cols = []\n\n    for col in dataframe.select_dtypes(include=\"object\").columns:\n        sample_values = dataframe[col].dropna().head(sample_size)\n        parse_attempts = sample_values.apply(\n            lambda x: pd.to_datetime(x, errors=\"coerce\")\n        )\n        success_ratio = parse_attempts.notna().mean()\n        if success_ratio &gt;= 0.8:  # 80% of values parse as dates\n            possible_date_cols.append(col)\n\n    return possible_date_cols\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_shape","title":"<code>show_shape(dataframe)</code>","text":"<p>Return the shape (number of rows and columns) of the given DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_shape--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_shape--returns","title":"Returns","text":"<p>tuple of int     A tuple containing the number of rows and columns in the DataFrame.</p> Source code in <code>jcds/eda/inspect.py</code> <pre><code>def show_shape(dataframe):\n    \"\"\"\n    Return the shape (number of rows and columns) of the given DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    tuple of int\n        A tuple containing the number of rows and columns in the DataFrame.\n\n    \"\"\"\n    return dataframe.shape\n</code></pre>"},{"location":"api/#column-utilities","title":"Column Utilities","text":""},{"location":"api/#jcds.eda.lists.get_cat_list","title":"<code>get_cat_list(dataframe)</code>","text":"<p>Return a list of categorical column names or the subset of the DataFrame containing only categorical columns.</p>"},{"location":"api/#jcds.eda.lists.get_cat_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to analyze. return_names : bool     If True, returns a list of column names. If False, returns a DataFrame slice.</p>"},{"location":"api/#jcds.eda.lists.get_cat_list--returns","title":"Returns","text":"<p>list of str or pd.DataFrame     Categorical column names if <code>return_names</code> is True, or a DataFrame containing only categorical columns if False.</p> Source code in <code>jcds/eda/lists.py</code> <pre><code>def get_cat_list(dataframe):\n    \"\"\"\n    Return a list of categorical column names or the subset of the DataFrame containing only categorical columns.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to analyze.\n    return_names : bool\n        If True, returns a list of column names. If False, returns a DataFrame slice.\n\n    Returns\n    -------\n    list of str or pd.DataFrame\n        Categorical column names if `return_names` is True, or a DataFrame containing only categorical columns if False.\n\n    \"\"\"\n\n    cat_list = dataframe.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n    return cat_list\n</code></pre>"},{"location":"api/#jcds.eda.lists.get_cont_list","title":"<code>get_cont_list(dataframe)</code>","text":"<p>Return a list of continuous (non-categorical) column names or the subset of the DataFrame containing only continuous columns.</p>"},{"location":"api/#jcds.eda.lists.get_cont_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to analyze. return_names : bool     If True, returns a list of column names. If False, returns a DataFrame slice.</p>"},{"location":"api/#jcds.eda.lists.get_cont_list--returns","title":"Returns","text":"<p>list of str or pd.DataFrame     Continuous column names if <code>return_names</code> is True, or a DataFrame containing only continuous columns if False.</p> Source code in <code>jcds/eda/lists.py</code> <pre><code>def get_cont_list(dataframe):\n    \"\"\"\n    Return a list of continuous (non-categorical) column names or the subset of the DataFrame containing only continuous columns.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to analyze.\n    return_names : bool\n        If True, returns a list of column names. If False, returns a DataFrame slice.\n\n    Returns\n    -------\n    list of str or pd.DataFrame\n        Continuous column names if `return_names` is True, or a DataFrame containing only continuous columns if False.\n\n    \"\"\"\n\n    cont_list = dataframe.select_dtypes(exclude=[\"category\", \"object\"]).columns.tolist()\n    return cont_list\n</code></pre>"},{"location":"api/#jcds.eda.lists.list_unique_values","title":"<code>list_unique_values(dataframe, column)</code>","text":"<p>Print the unique values in one or more specified columns of a DataFrame and display the corresponding code snippet.</p>"},{"location":"api/#jcds.eda.lists.list_unique_values--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. column : str or list of str     The column name or list of column names to inspect for unique values.</p>"},{"location":"api/#jcds.eda.lists.list_unique_values--returns","title":"Returns","text":"<p>None     This function prints output to the console and does not return a value.</p> Source code in <code>jcds/eda/lists.py</code> <pre><code>def list_unique_values(dataframe, column):\n    \"\"\"\n    Print the unique values in one or more specified columns of a DataFrame and display the corresponding code snippet.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    column : str or list of str\n        The column name or list of column names to inspect for unique values.\n\n    Returns\n    -------\n    None\n        This function prints output to the console and does not return a value.\n\n    \"\"\"\n    display_code = f'DataFrame[\"{column}\"].unique().tolist()'\n    print_code_line(display_code)\n\n    if isinstance(column, list):\n        for col in column:\n            print(f\"Unique values in '{col}':\")\n            print(dataframe[col].unique().tolist())\n            print(\"---\")\n    else:\n        print(f\"Unique values in '{column}':\")\n        print(dataframe[column].unique().tolist())\n</code></pre>"},{"location":"api/#datetime-utilities","title":"Datetime Utilities","text":""},{"location":"api/#jcds.eda.datetime.create_dt_col","title":"<code>create_dt_col(dataframe, datetime_col, col_type='month')</code>","text":"<p>Wrapper for create_dt_cols that creates a single datetime-derived column.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_col--parameters","title":"Parameters","text":"<p>dataframe : DataFrame     The input DataFrame. datetime_col : str     Name of the column containing datetime values. col_type : str     A single datetime component to extract.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_col--returns","title":"Returns","text":"<p>DataFrame     A copy of the DataFrame with one new datetime feature column added.</p> Source code in <code>jcds/eda/datetime.py</code> <pre><code>def create_dt_col(dataframe, datetime_col, col_type=\"month\"):\n    \"\"\"\n    Wrapper for create_dt_cols that creates a single datetime-derived column.\n\n    Parameters\n    ----------\n    dataframe : DataFrame\n        The input DataFrame.\n    datetime_col : str\n        Name of the column containing datetime values.\n    col_type : str\n        A single datetime component to extract.\n\n    Returns\n    -------\n    DataFrame\n        A copy of the DataFrame with one new datetime feature column added.\n\n    \"\"\"\n    return create_dt_cols(dataframe, datetime_col, [col_type])\n</code></pre>"},{"location":"api/#jcds.eda.datetime.create_dt_cols","title":"<code>create_dt_cols(dataframe, datetime_col, col_types=['month'])</code>","text":"<p>Add one or more datetime-derived columns to a DataFrame from a single datetime column.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--parameters","title":"Parameters","text":"<p>dataframe : DataFrame     The input DataFrame. datetime_col : str     Name of the column containing datetime values. col_types : str or list of str, default [\"month\"]     One or more datetime components to extract. Supported values:     \"year\", \"month\", \"day\", \"weekday\", \"weekday_name\", \"weekofyear\",     \"quarter\", \"is_weekend\", \"dayofyear\", \"is_month_start\", \"is_month_end\".</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--returns","title":"Returns","text":"<p>DataFrame     A copy of the DataFrame with the new datetime feature columns added.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--raises","title":"Raises","text":"<p>ValueError     If the datetime_col is missing or if any component in col_types is unsupported.</p> Source code in <code>jcds/eda/datetime.py</code> <pre><code>def create_dt_cols(dataframe, datetime_col, col_types=[\"month\"]):\n    \"\"\"\n    Add one or more datetime-derived columns to a DataFrame from a single datetime column.\n\n    Parameters\n    ----------\n    dataframe : DataFrame\n        The input DataFrame.\n    datetime_col : str\n        Name of the column containing datetime values.\n    col_types : str or list of str, default [\"month\"]\n        One or more datetime components to extract. Supported values:\n        \"year\", \"month\", \"day\", \"weekday\", \"weekday_name\", \"weekofyear\",\n        \"quarter\", \"is_weekend\", \"dayofyear\", \"is_month_start\", \"is_month_end\".\n\n    Returns\n    -------\n    DataFrame\n        A copy of the DataFrame with the new datetime feature columns added.\n\n    Raises\n    ------\n    ValueError\n        If the datetime_col is missing or if any component in col_types is unsupported.\n\n    \"\"\"\n    supported_types = {\n        \"year\": lambda x: x.dt.year,\n        \"month\": lambda x: x.dt.month,\n        \"day\": lambda x: x.dt.day,\n        \"weekday\": lambda x: x.dt.weekday,\n        \"weekday_name\": lambda x: x.dt.day_name(),\n        \"weekofyear\": lambda x: x.dt.isocalendar().week,\n        \"quarter\": lambda x: x.dt.quarter,\n        \"is_weekend\": lambda x: x.dt.weekday &gt;= 5,\n        \"dayofyear\": lambda x: x.dt.dayofyear,\n        \"is_month_start\": lambda x: x.dt.is_month_start,\n        \"is_month_end\": lambda x: x.dt.is_month_end,\n    }\n\n    if datetime_col not in dataframe.columns:\n        raise ValueError(f\"Column '{datetime_col}' not found in DataFrame.\")\n\n    if isinstance(col_types, str):\n        col_types = [col_types]\n\n    unsupported = [ct for ct in col_types if ct not in supported_types]\n    if unsupported:\n        raise ValueError(\n            f\"Unsupported col_type(s): {unsupported}. Must be one of: {list(supported_types)}\"\n        )\n\n    dataframe = dataframe.copy()\n\n    if not pd.api.types.is_datetime64_any_dtype(dataframe[datetime_col]):\n        # Check for consistent format\n        sample = dataframe[datetime_col].dropna().astype(str)\n        has_dash = sample.str.contains(\"-\").any()\n        has_slash = sample.str.contains(\"/\").any()\n\n        if has_dash and has_slash:\n            raise ValueError(\n                f\"Inconsistent datetime format detected in column '{datetime_col}'. \"\n                \"Mix of '-' and '/' found. Please standardize format before applying datetime expansion.\"\n            )\n\n        try:\n            dataframe[datetime_col] = pd.to_datetime(dataframe[datetime_col])\n        except Exception as e:\n            raise ValueError(f\"Could not convert '{datetime_col}' to datetime: {e}\")\n\n    for col_type in col_types:\n        new_col = f\"{datetime_col}_{col_type}\"\n        dataframe[new_col] = supported_types[col_type](dataframe[datetime_col])\n\n    return dataframe\n</code></pre>"},{"location":"api/#quick-reports","title":"Quick Reports","text":""},{"location":"api/#jcds.eda.reports.display_all_col_head","title":"<code>display_all_col_head(dataframe, head=5)</code>","text":"<p>Display the first few rows of a DataFrame with all columns visible.</p> <p>Temporarily adjusts the pandas display settings to ensure all columns are shown, regardless of the total number, and uses IPython's <code>display()</code> function for clean notebook output.</p>"},{"location":"api/#jcds.eda.reports.display_all_col_head--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to display. head : int, optional     The number of rows to show from the top of the DataFrame. Defaults to 5.</p>"},{"location":"api/#jcds.eda.reports.display_all_col_head--notes","title":"Notes","text":"<ul> <li>Temporarily sets the pandas display option to show all columns.</li> <li>Uses IPython's <code>display()</code> function for cleaner notebook output.</li> </ul>"},{"location":"api/#jcds.eda.reports.display_all_col_head--returns","title":"Returns","text":"<p>None     This function prints to the notebook interface and does not return a value.</p> Source code in <code>jcds/eda/reports.py</code> <pre><code>def display_all_col_head(dataframe, head=5):\n    \"\"\"\n    Display the first few rows of a DataFrame with all columns visible.\n\n    Temporarily adjusts the pandas display settings to ensure all columns are shown, regardless of the total number,\n    and uses IPython's `display()` function for clean notebook output.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to display.\n    head : int, optional\n        The number of rows to show from the top of the DataFrame. Defaults to 5.\n\n    Notes\n    -----\n    - Temporarily sets the pandas display option to show all columns.\n    - Uses IPython's `display()` function for cleaner notebook output.\n\n    Returns\n    -------\n    None\n        This function prints to the notebook interface and does not return a value.\n\n    \"\"\"\n\n    with pd.option_context(\"display.max_columns\", None):\n        display(dataframe.head(head))\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.dqr_cat","title":"<code>dqr_cat(dataframe)</code>","text":"<p>Generate a data quality report for categorical features in a given DataFrame.</p> <p>This function calculates and prints the following metrics for each feature in the provided list: - Total count of non-missing values - Total count of missing values - Percentage of missing values - Cardinality (number of unique values) - Mode 1 (most frequent value), its frequency, and percentage - Mode 2 (second most frequent value), its frequency, and percentage - Descriptive statistics for each feature</p>"},{"location":"api/#jcds.eda.reports.dqr_cat--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The DataFrame containing the data to be analyzed. list_of_features : list of str     The list of column names (features) for which the data quality report is generated.</p>"},{"location":"api/#jcds.eda.reports.dqr_cat--returns","title":"Returns","text":"<p>None     This function prints the data quality report to the console.</p> Source code in <code>jcds/eda/reports.py</code> <pre><code>def dqr_cat(dataframe):\n    \"\"\"\n    Generate a data quality report for categorical features in a given DataFrame.\n\n    This function calculates and prints the following metrics for each feature in the provided list:\n    - Total count of non-missing values\n    - Total count of missing values\n    - Percentage of missing values\n    - Cardinality (number of unique values)\n    - Mode 1 (most frequent value), its frequency, and percentage\n    - Mode 2 (second most frequent value), its frequency, and percentage\n    - Descriptive statistics for each feature\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The DataFrame containing the data to be analyzed.\n    list_of_features : list of str\n        The list of column names (features) for which the data quality report is generated.\n\n    Returns\n    -------\n    None\n        This function prints the data quality report to the console.\n\n    \"\"\"\n\n    # Initialize variables\n    round_to = 2\n    list_feature_name = []\n    list_count = []\n    list_missing = []\n    list_percent = []\n    list_cardinality = []\n    list_mode1 = []\n    list_mode1_freq = []\n    list_mode1_perc = []\n    list_mode2 = []\n    list_mode2_freq = []\n    list_mode2_perc = []\n\n    # Create list of non-categorical values\n    list_of_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n\n    # Total rows\n    total_rows = dataframe.shape[0]\n\n    if len(list_of_features) == 0:\n        print(\"This dataset does not have any categorical columns.\")\n        return\n\n    print(\"The categorical features are: \")\n    print(list_of_features)\n\n    for feature in list_of_features:\n\n        total_count = dataframe[feature].count()\n        total_missing = dataframe[feature].isnull().sum()\n        percent_missing = np.round(total_missing / total_rows * 100, round_to)\n        cardinality = len(dataframe[feature].unique())\n\n        # Use value counts to get modes\n        results = dataframe[feature].value_counts()\n        # Calculate mode\n        mode1_name = results.index[0]\n        mode1_count = results.iloc[0]\n        mode1_percent = np.round((mode1_count / total_count) * 100, round_to)\n\n        # Initialize mode 2 variables\n        mode2_name = None\n        mode2_count = 0\n        mode2_percent = 0.0\n\n        # Calculate 2nd mode if it exists\n        if len(results) &gt; 1:\n            mode2_name = results.index[1]\n            mode2_count = results.iloc[1]\n            mode2_percent = np.round((mode2_count / total_count) * 100, round_to)\n\n        # Append results to lists\n        list_feature_name.append(feature)\n        list_count.append(total_count)\n        list_missing.append(total_missing)\n        list_percent.append(percent_missing)\n        list_cardinality.append(cardinality)\n        list_mode1.append(mode1_name)\n        list_mode1_freq.append(mode1_count)\n        list_mode1_perc.append(mode1_percent)\n        list_mode2.append(mode2_name)\n        list_mode2_freq.append(mode2_count)\n        list_mode2_perc.append(mode2_percent)\n\n    # Create dataframes\n    data = {\n        \"Feature\": list_feature_name,\n        \"Count\": list_count,\n        \"Missing\": list_missing,\n        \"% Missing\": list_percent,\n        \"Cardinality\": list_cardinality,\n    }\n\n    data_mode1 = {\n        \"Feature\": list_feature_name,\n        \"Mode 1\": list_mode1,\n        \"Mode 1 Freq.\": list_mode1_freq,\n        \"Mode 1 %\": list_mode1_perc,\n    }\n\n    data_mode2 = {\n        \"Feature\": list_feature_name,\n        \"Mode 2\": list_mode2,\n        \"Mode 2 Freq.\": list_mode2_freq,\n        \"Mode 2 %\": list_mode2_perc,\n    }\n\n    df = pd.DataFrame(data)\n    df1 = pd.DataFrame(data_mode1)\n    df2 = pd.DataFrame(data_mode2)\n\n    # Get descriptive statistics and transpose\n    stats = dataframe[list_of_features].describe(include=\"object\")\n    transposed_stats = stats.T\n\n    # Print results\n    print(\"Data Quality Report for Categorical Features\")\n    print(f\"Total features: {len(list_of_features)} / {total_rows} rows\")\n    print(\"============================================\")\n    print(\"Stats\")\n    print(\"-----\")\n    display(df)\n\n    print(\"\\n\")\n    print(\"Mode 1\")\n    print(\"------\")\n    display(df1)\n\n    print(\"\\n\")\n    print(\"Mode 2\")\n    print(\"------\")\n    display(df2)\n\n    print(\"\\n\")\n    print(\"Descriptive Stats\")\n    print(\"-----------------\")\n    display(transposed_stats)\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.dqr_cont","title":"<code>dqr_cont(dataframe)</code>","text":"<p>Generate a data quality report for continuous features in a given DataFrame.</p> <p>This function calculates and prints the following metrics for each feature in the provided list: - Total count of non-missing values - Total count of missing values - Percentage of missing values - Cardinality (number of unique values) - Descriptive statistics (mean, standard deviation, min, max)</p>"},{"location":"api/#jcds.eda.reports.dqr_cont--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The DataFrame containing the data to be analyzed. list_of_features : list of str     The list of column names (features) for which the data quality report is generated.</p>"},{"location":"api/#jcds.eda.reports.dqr_cont--returns","title":"Returns","text":"<p>None     This function prints the data quality report to the console.</p> Source code in <code>jcds/eda/reports.py</code> <pre><code>def dqr_cont(dataframe):\n    \"\"\"\n    Generate a data quality report for continuous features in a given DataFrame.\n\n    This function calculates and prints the following metrics for each feature in the provided list:\n    - Total count of non-missing values\n    - Total count of missing values\n    - Percentage of missing values\n    - Cardinality (number of unique values)\n    - Descriptive statistics (mean, standard deviation, min, max)\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The DataFrame containing the data to be analyzed.\n    list_of_features : list of str\n        The list of column names (features) for which the data quality report is generated.\n\n    Returns\n    -------\n    None\n        This function prints the data quality report to the console.\n\n    \"\"\"\n\n    # Initialize variables\n    round_to = 2\n    list_feature_name = []\n    list_count = []\n    list_missing = []\n    list_percent = []\n    list_cardinality = []\n\n    # Create list of non-categorical values\n    list_of_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n\n    # Total rows\n    total_rows = dataframe.shape[0]\n\n    if len(list_of_features) == 0:\n        print(\"This dataset does not have any non-categorical features.\")\n        return\n\n    print(\"The non-categorical features are: \")\n    print(list_of_features)\n\n    for feature in list_of_features:\n        # Get stats for each feature\n        total_count = dataframe[feature].count()\n        total_missing = dataframe[feature].isnull().sum()\n        percent_missing = total_missing / total_rows * 100\n        cardinality = len(dataframe[feature].unique())\n\n        # Append result to variables\n        list_feature_name.append(feature)\n        list_count.append(total_count)\n        list_missing.append(total_missing)\n        list_percent.append(np.round(percent_missing, round_to))\n        list_cardinality.append(cardinality)\n\n    # Create dataframe\n    data = {\n        \"Feature\": list_feature_name,\n        \"Count\": list_count,\n        \"Missing\": list_missing,\n        \"% missing\": list_percent,\n        \"Cardinality\": list_cardinality,\n    }\n    df = pd.DataFrame(data)\n\n    # Get descriptive statistics and transpose\n    stats = np.round(dataframe[list_of_features].describe(), round_to)\n    transposed_stats = stats.T\n\n    # Print results\n    print(\"Data Quality for Continous Features\")\n    print(f\"Total Features: {len(list_of_features)} / {total_rows} rows\")\n    display(df)\n\n    print(\"\\n\")\n    print(\"Descriptive Stats\")\n    display(transposed_stats)\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.long_report","title":"<code>long_report(dataframe)</code>","text":"<p>Generate a detailed summary report of a pandas DataFrame, including shape, missing value statistics, and a breakdown of categorical and continuous features with their unique value counts.</p> <p>This function prints: - Total number of columns and rows - Number and percentage of rows that are entirely missing - Number and percentage of columns with any missing values - Total number of missing values in the dataset - Count of categorical and continuous features - For each categorical and continuous feature: number of unique values</p>"},{"location":"api/#jcds.eda.reports.long_report--notes","title":"Notes","text":"<ul> <li>Categorical features are detected based on 'object' and 'category' dtypes.</li> <li>Continuous features include all other non-categorical dtypes.</li> <li>Missing value percentages are rounded to two decimal places.</li> <li>A placeholder for <code>.info(memory_usage='deep')</code> is printed but not executed.</li> </ul>"},{"location":"api/#jcds.eda.reports.long_report--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame to analyze.</p>"},{"location":"api/#jcds.eda.reports.long_report--returns","title":"Returns","text":"<p>None     This function prints the detailed summary report to the console.</p> Source code in <code>jcds/eda/reports.py</code> <pre><code>@deprecated(\n    reason=\"This will be replaced with a new function.\",\n    version=\"0.3.0\",\n)\ndef long_report(dataframe):\n    \"\"\"\n    Generate a detailed summary report of a pandas DataFrame, including shape, missing value statistics,\n    and a breakdown of categorical and continuous features with their unique value counts.\n\n    This function prints:\n    - Total number of columns and rows\n    - Number and percentage of rows that are entirely missing\n    - Number and percentage of columns with any missing values\n    - Total number of missing values in the dataset\n    - Count of categorical and continuous features\n    - For each categorical and continuous feature: number of unique values\n\n    Notes\n    -----\n    - Categorical features are detected based on 'object' and 'category' dtypes.\n    - Continuous features include all other non-categorical dtypes.\n    - Missing value percentages are rounded to two decimal places.\n    - A placeholder for `.info(memory_usage='deep')` is printed but not executed.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze.\n\n    Returns\n    -------\n    None\n        This function prints the detailed summary report to the console.\n\n    \"\"\"\n\n    ROUND = 2\n    # Get features not labeled as categorical\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get features labeled as categorical\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get shape, total rows and cols\n    total_rows = dataframe.shape[0]\n    total_cols = dataframe.shape[1]\n    # Count/% rows that are missing values\n    rows_with_all_na = dataframe.isna().all(axis=1).sum()\n    percent_na_rows = np.round(rows_with_all_na / total_rows * 100, ROUND)\n    # Count/% cols that are missing values\n    cols_with_na = dataframe.isna().any(axis=0).sum()\n    percent_na_cols = np.round(cols_with_na / total_cols * 100, ROUND)\n    total_na = dataframe.isna().sum().sum()\n\n    print(\"============================================\")\n    print(\"Quick Report - info(memory_usage='deep')\")\n    print(f\"Total cols: {total_cols}\")\n    print(f\"Rows missing all values: {rows_with_all_na} ({percent_na_rows}%)\")\n    print(f\"Total Rows: {total_rows}\")\n    print(f\"Cols with missing values: {cols_with_na} ({percent_na_cols}%)\")\n    print(f\"Total missing values in dataset: {total_na}\")\n    print(\"============================================\")\n    print(f\"Categorical features: {len(cat_features)}\")\n    for cat in cat_features:\n        num_unique = dataframe[cat].unique()\n        print(f\"- {cat}: {len(num_unique)} unique values\")\n    print(\"============================================\")\n    print(f\"Continuous features: {len(cont_features)}\")\n    for cont in cont_features:\n        num_unique = dataframe[cont].unique()\n        print(f\"- {cont}: {len(num_unique)} unique values\")\n\n    # info = dataframe.info(memory_usage='deep')\n    # display(info)\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.quick_report","title":"<code>quick_report(dataframe)</code>","text":"<p>Generate a quick summary report of a pandas DataFrame, including shape, missing value statistics, and memory usage.</p> <p>This function prints: - Total number of columns and rows - Number and percentage of rows that are entirely missing - Number and percentage of columns that have any missing values - Total number of missing values in the dataset - Output of <code>DataFrame.info()</code> with memory usage (<code>deep=True</code>)</p>"},{"location":"api/#jcds.eda.reports.quick_report--notes","title":"Notes","text":"<ul> <li>Categorical and continuous feature identification is performed but not displayed.</li> <li>Missing value calculations are rounded to two decimal places.</li> </ul>"},{"location":"api/#jcds.eda.reports.quick_report--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame to analyze.</p>"},{"location":"api/#jcds.eda.reports.quick_report--returns","title":"Returns","text":"<p>None     This function prints the summary report to the console.</p> Source code in <code>jcds/eda/reports.py</code> <pre><code>@deprecated(\n    reason=\"This will be replaced with a new function.\",\n    version=\"0.3.0\",\n)\ndef quick_report(dataframe):\n    \"\"\"\n    Generate a quick summary report of a pandas DataFrame, including shape, missing value statistics, and memory usage.\n\n    This function prints:\n    - Total number of columns and rows\n    - Number and percentage of rows that are entirely missing\n    - Number and percentage of columns that have any missing values\n    - Total number of missing values in the dataset\n    - Output of `DataFrame.info()` with memory usage (`deep=True`)\n\n    Notes\n    -----\n    - Categorical and continuous feature identification is performed but not displayed.\n    - Missing value calculations are rounded to two decimal places.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze.\n\n    Returns\n    -------\n    None\n        This function prints the summary report to the console.\n\n    \"\"\"\n\n    ROUND = 2\n    # Get features not labeled as categorical\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get features labeled as categorical\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get shape, total rows and cols\n    total_rows = dataframe.shape[0]\n    total_cols = dataframe.shape[1]\n    # Count/% rows that are missing values\n    rows_with_all_na = dataframe.isna().all(axis=1).sum()\n    percent_na_rows = np.round(rows_with_all_na / total_rows * 100, ROUND)\n    # Count/% cols that are missing values\n    cols_with_na = dataframe.isna().any(axis=0).sum()\n    percent_na_cols = np.round(cols_with_na / total_cols * 100, ROUND)\n    total_na = dataframe.isna().sum().sum()\n\n    print(\"============================================\")\n    print(\"Quick Report - info(memory_usage='deep')\")\n    print(f\"Total cols: {total_cols}\")\n    print(f\"Rows missing all values: {rows_with_all_na} ({percent_na_rows}%)\")\n    print(f\"Total Rows: {total_rows}\")\n    print(f\"Cols with missing values: {cols_with_na} ({percent_na_cols}%)\")\n    print(f\"Total missing values in dataset: {total_na}\")\n    print(\"============================================\")\n</code></pre>"},{"location":"api/#data-inputoutput","title":"Data Input/Output","text":""},{"location":"api/#jcds.dataio.load_csv","title":"<code>load_csv(filepath: Union[str, Path], encodings: Optional[list[str]] = None, preview_bytes: int = 300, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Attempt to load a CSV using multiple encodings. If all fail, preview raw bytes.</p>"},{"location":"api/#jcds.dataio.load_csv--parameters","title":"Parameters","text":"<p>filepath : str or Path     Path to the CSV file. encodings : list of str, optional     List of encodings to try. Defaults to common ones. preview_bytes : int     Number of bytes to show if loading fails (for debugging). **kwargs     Additional arguments passed to <code>pd.read_csv</code>.</p>"},{"location":"api/#jcds.dataio.load_csv--returns","title":"Returns","text":"<p>pd.DataFrame     Loaded DataFrame.</p>"},{"location":"api/#jcds.dataio.load_csv--raises","title":"Raises","text":"<p>UnicodeDecodeError     If no encoding successfully loads the file.</p>"},{"location":"api/#jcds.dataio.load_csv--notes","title":"Notes","text":"Source code in <code>jcds/dataio/io_utils.py</code> <pre><code>def load_csv(\n    filepath: Union[str, Path],\n    encodings: Optional[list[str]] = None,\n    preview_bytes: int = 300,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Attempt to load a CSV using multiple encodings. If all fail, preview raw bytes.\n\n    Parameters\n    ----------\n    filepath : str or Path\n        Path to the CSV file.\n    encodings : list of str, optional\n        List of encodings to try. Defaults to common ones.\n    preview_bytes : int\n        Number of bytes to show if loading fails (for debugging).\n    **kwargs\n        Additional arguments passed to `pd.read_csv`.\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded DataFrame.\n\n    Raises\n    ------\n    UnicodeDecodeError\n        If no encoding successfully loads the file.\n\n    Notes\n    -----\n    \"\"\"\n    filepath = Path(filepath)\n    encodings = encodings or [\"utf-8\", \"utf-8-sig\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n\n    for encoding in encodings:\n        try:\n            dataframe = pd.read_csv(filepath, encoding=encoding, **kwargs)\n            print(f\"[jcds] Loaded CSV with encoding: {encoding}\")\n            return dataframe\n        except UnicodeDecodeError:\n            print(f\"[jcds] Failed to load with encoding: {encoding}\")\n            continue\n        except Exception as e:\n            print(f\"[jcds] Unexpected error with encoding '{encoding}': {e}\")\n            continue\n\n    print(f\"[jcds] \u274c Could not decode file: {filepath}\")\n    print(f\"[jcds] Preview of raw bytes:\")\n\n    try:\n        with open(filepath, \"rb\") as f:\n            raw = f.read(preview_bytes)\n            print(raw.decode(\"latin1\", errors=\"replace\"))  # fallback preview\n    except Exception as e:\n        print(f\"[jcds] Also failed to read raw bytes: {e}\")\n\n    raise ValueError(\n        f\"[jcds] Failed to decode file {filepath} with encodings {encodings}. \"\n        \"See preview above for troubleshooting.\"\n    )\n</code></pre>"},{"location":"api/#jcds.dataio.load_parquet","title":"<code>load_parquet(filepath: Union[str, Path], **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Load a Parquet file into a pandas DataFrame.</p>"},{"location":"api/#jcds.dataio.load_parquet--parameters","title":"Parameters","text":"<p>filepath : str or Path     Path to the Parquet file. **kwargs     Additional keyword arguments passed to <code>pd.read_parquet</code>.</p>"},{"location":"api/#jcds.dataio.load_parquet--returns","title":"Returns","text":"<p>pd.DataFrame     The loaded DataFrame.</p>"},{"location":"api/#jcds.dataio.load_parquet--notes","title":"Notes","text":"<p>Requires either <code>pyarrow</code> or <code>fastparquet</code> to be installed.</p> Source code in <code>jcds/dataio/io_utils.py</code> <pre><code>def load_parquet(filepath: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Load a Parquet file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    filepath : str or Path\n        Path to the Parquet file.\n    **kwargs\n        Additional keyword arguments passed to `pd.read_parquet`.\n\n    Returns\n    -------\n    pd.DataFrame\n        The loaded DataFrame.\n\n    Notes\n    -----\n    Requires either `pyarrow` or `fastparquet` to be installed.\n\n    \"\"\"\n    filepath = Path(filepath)\n    return pd.read_parquet(filepath, **kwargs)\n</code></pre>"},{"location":"api/#jcds.dataio.read_s3","title":"<code>read_s3(bucket_name: str, file_name: str, file_type: str = 'csv') -&gt; pd.DataFrame</code>","text":"<p>Download a public file from an S3 bucket and load it into a pandas DataFrame.</p>"},{"location":"api/#jcds.dataio.read_s3--parameters","title":"Parameters","text":"<p>bucket_name : str     The name of the public S3 bucket. file_name : str     The path to the file within the bucket. file_type : str, optional     The type of file to load. Supported values are 'csv' and 'excel'. Defaults to 'csv'.</p>"},{"location":"api/#jcds.dataio.read_s3--returns","title":"Returns","text":"<p>pd.DataFrame or None     The loaded DataFrame if successful, otherwise None.</p>"},{"location":"api/#jcds.dataio.read_s3--notes","title":"Notes","text":"<ul> <li>This function assumes the file is publicly accessible via a standard S3 URL.</li> <li>For Excel files, only <code>.xlsx</code> is supported.</li> <li>Requires the <code>requests</code>, <code>pandas</code>, and <code>io</code> modules.</li> <li>Prints an error message and returns None if the request fails or the file type is unsupported.</li> </ul> Source code in <code>jcds/dataio/s3_io.py</code> <pre><code>def read_s3(bucket_name: str, file_name: str, file_type: str = \"csv\") -&gt; pd.DataFrame:\n    \"\"\"Download a public file from an S3 bucket and load it into a pandas DataFrame.\n\n    Parameters\n    ----------\n    bucket_name : str\n        The name of the public S3 bucket.\n    file_name : str\n        The path to the file within the bucket.\n    file_type : str, optional\n        The type of file to load. Supported values are 'csv' and 'excel'. Defaults to 'csv'.\n\n    Returns\n    -------\n    pd.DataFrame or None\n        The loaded DataFrame if successful, otherwise None.\n\n    Notes\n    -----\n    - This function assumes the file is publicly accessible via a standard S3 URL.\n    - For Excel files, only `.xlsx` is supported.\n    - Requires the `requests`, `pandas`, and `io` modules.\n    - Prints an error message and returns None if the request fails or the file type is unsupported.\n\n    \"\"\"\n    url = f\"https://{bucket_name}.s3.amazonaws.com/{file_name}\"\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n\n        if file_type == \"csv\":\n            dataframe = pd.read_csv(BytesIO(response.content))\n        elif file_type == \"excel\":\n            dataframe = pd.read_excel(BytesIO(response.content))\n        else:\n            raise ValueError(\"Unsupported file type. Use 'csv' or 'excel'.\")\n        return dataframe\n\n    except ValueError:\n        raise  # Let unsupported type errors propagate for testing\n\n    except Exception as e:\n        print(f\"[jcds] Error loading file from S3: {e}\")\n        return None\n</code></pre>"},{"location":"api/#jcds.dataio.save_csv","title":"<code>save_csv(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None</code>","text":"<p>Save a DataFrame to a CSV file.</p>"},{"location":"api/#jcds.dataio.save_csv--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to save. filepath : str or Path     Destination file path. **kwargs     Additional keyword arguments passed to <code>DataFrame.to_csv</code>.</p>"},{"location":"api/#jcds.dataio.save_csv--notes","title":"Notes","text":"<p>Automatically creates the parent directory if it doesn't exist. Uses <code>index=False</code> by default unless overridden.</p> Source code in <code>jcds/dataio/io_utils.py</code> <pre><code>def save_csv(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None:\n    \"\"\"Save a DataFrame to a CSV file.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to save.\n    filepath : str or Path\n        Destination file path.\n    **kwargs\n        Additional keyword arguments passed to `DataFrame.to_csv`.\n\n    Notes\n    -----\n    Automatically creates the parent directory if it doesn't exist.\n    Uses `index=False` by default unless overridden.\n\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    # Default to index=False if not explicitly passed\n    if \"index\" not in kwargs:\n        kwargs[\"index\"] = False\n\n    dataframe.to_csv(filepath, **kwargs)\n</code></pre>"},{"location":"api/#jcds.dataio.save_parquet","title":"<code>save_parquet(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None</code>","text":"<p>Save a DataFrame to a Parquet file.</p>"},{"location":"api/#jcds.dataio.save_parquet--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to save. filepath : str or Path     Destination file path. **kwargs     Additional keyword arguments passed to <code>DataFrame.to_parquet</code>.</p>"},{"location":"api/#jcds.dataio.save_parquet--notes","title":"Notes","text":"<p>Automatically creates the parent directory if it doesn't exist.</p> Source code in <code>jcds/dataio/io_utils.py</code> <pre><code>def save_parquet(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None:\n    \"\"\"Save a DataFrame to a Parquet file.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to save.\n    filepath : str or Path\n        Destination file path.\n    **kwargs\n        Additional keyword arguments passed to `DataFrame.to_parquet`.\n\n    Notes\n    -----\n    Automatically creates the parent directory if it doesn't exist.\n\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_parquet(filepath, index=False, **kwargs)\n</code></pre>"},{"location":"api/#reports","title":"Reports","text":""},{"location":"api/#jcds.reports.data_cardinality","title":"<code>data_cardinality(dataframe, show_columns=False)</code>","text":"<p>Summarizes the cardinality of the columns in the dataset.</p>"},{"location":"api/#jcds.reports.data_cardinality--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The input dataset.</p> bool, optional <p>Whether to display the list of columns in each category.</p>"},{"location":"api/#jcds.reports.data_cardinality--returns","title":"Returns","text":"<p>None     Prints summary information to the console.</p> Source code in <code>jcds/reports/reports.py</code> <pre><code>def data_cardinality(dataframe, show_columns=False):\n    \"\"\"\n    Summarizes the cardinality of the columns in the dataset.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The input dataset.\n\n    show_columns : bool, optional\n        Whether to display the list of columns in each category.\n\n    Returns\n    -------\n    None\n        Prints summary information to the console.\n    \"\"\"\n    print(\"CARDINALITY:\")\n    binary_list = show_binary_list(dataframe)\n    for key, value in binary_list.items():\n        total = len(value)\n        print(f\"{key}: {total} columns\")\n        if show_columns:\n            print(f\"\\tColumns: {value}\")\n\n    lowcardvars = show_lowcardvars(dataframe)\n    print(f\"\\tThere are {len(lowcardvars)} low cardinality variables.\")\n    if show_columns:\n        print(f\"\\tColumns: {lowcardvars}\")\n\n    highcardvars = show_highcardvars(dataframe)\n    print(f\"\\tThere are {len(highcardvars)} high cardinality variables.\")\n    if show_columns:\n        print(f\"\\tColumns: {highcardvars}\")\n</code></pre>"},{"location":"api/#jcds.reports.data_info","title":"<code>data_info(dataframe, show_columns=False)</code>","text":"<p>Summarize the dataset's shape, memory usage, duplicates, and variable types.</p>"},{"location":"api/#jcds.reports.data_info--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The input dataset.</p> bool, optional <p>Whether to display the list of columns in each category.</p>"},{"location":"api/#jcds.reports.data_info--returns","title":"Returns","text":"<p>None     Prints summary information to the console.</p> Source code in <code>jcds/reports/reports.py</code> <pre><code>def data_info(dataframe, show_columns=False):\n    \"\"\"\n    Summarize the dataset's shape, memory usage, duplicates, and variable types.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The input dataset.\n\n    show_columns : bool, optional\n        Whether to display the list of columns in each category.\n\n    Returns\n    -------\n    None\n        Prints summary information to the console.\n    \"\"\"\n\n    print(\"\\nSHAPE:\")\n    memory_use = show_memory_use(dataframe)\n    shape = show_shape(dataframe)\n    print(f\"There are {shape[0]} rows and {shape[1]} columns ({memory_use:.2f} MB).\")\n\n    print(\"\\nDUPLICATES:\")\n    dupes = show_dupes(dataframe)\n    print(f\"There are {dupes} duplicated rows.\")\n\n    print(\"\\nCOLUMNS/VARIABLES:\")\n\n    print(\"Column dType Summary:\")\n    dtype_summary = get_dtype_summary(dataframe)\n    for key, value in dtype_summary.items():\n        if value &gt; 0:\n            print(f\" * {key}: {value}\")\n\n    convar = show_convar(dataframe)\n    print(f\"There are {len(convar)} numerical (int/float/bool) variables.\")\n    if show_columns:\n        print(f\"\\tColumns: {convar}\")\n\n    catvar = show_catvar(dataframe)\n    print(f\"There are {len(catvar)} categorical (nominal/ordinal) variables.\")\n    if show_columns:\n        print(f\"\\tColumns: {catvar}\")\n\n    print(\"\\nDATETIME COLUMNS:\")\n    dt_cols = show_datetime_columns(dataframe)\n    possible_dt_cols = show_possible_datetime_columns(dataframe)\n    print(\n        f\"There are {len(dt_cols)} datetime variables and {len(possible_dt_cols)} possible datetime variables.\"\n    )\n\n    print(\"\\nOTHER COLUMN/VARIABLE INFO:\")\n    id_like_columns = count_id_like_columns(dataframe)\n    print(f\"ID Like Columns: {id_like_columns}\")\n\n    mixed_columns = show_mixed_type_columns(dataframe)\n    print(f\"Mixed dType Columns: {len(mixed_columns)}\")\n    if show_columns:\n        print(f\"\\tColumns: {mixed_columns}\")\n</code></pre>"},{"location":"api/#aws","title":"AWS","text":""},{"location":"api/#jcds.aws.list_s3_bucket","title":"<code>list_s3_bucket(bucket_name)</code>","text":"<p>List the contents (object keys) of a public Amazon S3 bucket using anonymous access.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--parameters","title":"Parameters","text":"<p>bucket_name : str     The name of the public S3 bucket to list contents from.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--returns","title":"Returns","text":"<p>None     Prints the keys (filenames) of the objects found in the bucket.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--notes","title":"Notes","text":"<ul> <li>This function uses anonymous (unsigned) access and works only with public buckets.</li> <li>Requires the <code>boto3</code> and <code>botocore</code> libraries.</li> <li>If the bucket is not public or an error occurs, an error message is printed.</li> </ul> Source code in <code>jcds/aws/s3_utils.py</code> <pre><code>def list_s3_bucket(bucket_name):\n    \"\"\"\n    List the contents (object keys) of a public Amazon S3 bucket using anonymous access.\n\n    Parameters\n    ----------\n    bucket_name : str\n        The name of the public S3 bucket to list contents from.\n\n    Returns\n    -------\n    None\n        Prints the keys (filenames) of the objects found in the bucket.\n\n    Notes\n    -----\n    - This function uses anonymous (unsigned) access and works only with public buckets.\n    - Requires the `boto3` and `botocore` libraries.\n    - If the bucket is not public or an error occurs, an error message is printed.\n\n    \"\"\"\n\n    try:\n        import boto3\n        from botocore import UNSIGNED\n        from botocore.config import Config\n    except ImportError as e:\n        print(\"Required library is not installed:\", e)\n        return\n\n    # create anonymous s3 client\n    s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n\n    try:\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        for obj in response.get(\"Contents\", []):\n            print(obj[\"Key\"])\n    except Exception as e:\n        print(\"Error accessing bucket:\", e)\n</code></pre>"},{"location":"api/#jcds-package-overview","title":"jcds Package Overview","text":"<p>options: members: - help</p>"},{"location":"api/#jcds.help","title":"<code>help(func_name=None, namespace=None)</code>","text":"<p>Unified help system for the jcds package.</p>"},{"location":"api/#jcds.help--parameters","title":"Parameters","text":"<p>func_name : str or None     If provided, shows the docstring for the named function.     If None, lists all available public functions. namespace : dict or None     Internal override used to pass a custom namespace.     If None, collects all callable functions from jcds submodules.</p> Source code in <code>jcds/__init__.py</code> <pre><code>def help(func_name=None, namespace=None):\n    \"\"\"\n    Unified help system for the jcds package.\n\n    Parameters\n    ----------\n    func_name : str or None\n        If provided, shows the docstring for the named function.\n        If None, lists all available public functions.\n    namespace : dict or None\n        Internal override used to pass a custom namespace.\n        If None, collects all callable functions from jcds submodules.\n    \"\"\"\n    import inspect as pyinspect\n    from jcds import eda, aws, dataio\n\n    if namespace is None:\n        namespace = {}\n\n        # Dynamically collect public callables from submodules\n        for mod in [eda, aws, dataio]:\n            for name in dir(mod):\n                # Skip all dunder names like __file__, __name__, etc.\n                if name.startswith(\"__\") and name.endswith(\"__\"):\n                    continue\n                obj = getattr(mod, name)\n                if callable(obj):\n                    namespace[name] = obj\n\n    functions = {name: obj for name, obj in namespace.items() if name != \"help\"}\n\n    if func_name is None:\n        print(\"Available functions in jcds:\\n\")\n        for name in sorted(functions):\n            print(f\"  - {name}\")\n        print('\\nUse jcds.help(\"function_name\") to see its documentation.')\n    else:\n        func = functions.get(func_name)\n        if func:\n            print(f\"\\nHelp for '{func_name}':\\n\")\n            print(pyinspect.getdoc(func) or \"(No docstring provided)\")\n        else:\n            print(f\"Function '{func_name}' not found.\")\n</code></pre>"}]}