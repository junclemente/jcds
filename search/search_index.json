{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to jcds \ud83e\uddea","text":"<p>jcds is a personal collection of reusable Python functions for data science workflows, built to save time and reduce repetition in Jupyter notebooks.</p> <p>It provides: - \ud83d\udd0e Quick EDA tools (like <code>quick_report</code> and <code>long_report</code>) - \ud83d\udcca Helpers for working with categorical and continuous variables - \ud83e\uddf9 Utilities for inspecting, cleaning, and summarizing data - \u2601\ufe0f Optional AWS integration for working with S3 - \ud83e\uddea Well-tested functions using <code>pytest</code></p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#1-installation","title":"1. Installation","text":"<p>Install the latest version directly from GitHub:</p> <pre><code>pip install git+https://github.com/junclemente/jcds.git\n</code></pre> <p>Or with AWS extras: </p> <pre><code>pip install git+https://github.com/junclemente/jcds.git[aws]\n</code></pre>"},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>import pandas as pd\nimport jcds.eda as jeda \n\ndf = pd.read_csv(\"your_dataset.csv\")\nduplicates = jeda.show_dupes(df)\nprint(duplicates)\n\nunique_values_list = jeda.count_unique_values(df, ['col1', 'col2', 'col3'])\nprint(unique_values_list)\n\nunique_values_column = jeda.count_unique_values(df, 'col')\nprint(unique_values_column)\n\n</code></pre> <p>Check out the API Reference to explore all available functions.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#eda-inspect","title":"EDA Inspect","text":""},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na","title":"<code>count_cols_with_all_na(dataframe)</code>","text":"<p>Count the number of columns in the DataFrame where all values are missing (NaN).</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_all_na--returns","title":"Returns","text":"<p>int     The number of columns where every row is NaN.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_cols_with_all_na(dataframe):\n    \"\"\"\n    Count the number of columns in the DataFrame where all values are missing (NaN).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of columns where every row is NaN.\n\n    \"\"\"\n\n    return dataframe.isna().all(axis=0).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na","title":"<code>count_cols_with_any_na(dataframe)</code>","text":"<p>Count the number of columns in the DataFrame that contain at least one missing (NaN) value.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_cols_with_any_na--returns","title":"Returns","text":"<p>int     The number of columns with at least one NaN value.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_cols_with_any_na(dataframe):\n    \"\"\"\n    Count the number of columns in the DataFrame that contain at least one missing (NaN) value.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of columns with at least one NaN value.\n\n    \"\"\"\n\n    return dataframe.isna().any(axis=0).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_id_like_columns","title":"<code>count_id_like_columns(dataframe, threshold=0.95)</code>","text":"<p>Count number of columns with high uniqueness (e.g., IDs).</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_id_like_columns(dataframe, threshold=0.95):\n    \"\"\"\n    Count number of columns with high uniqueness (e.g., IDs).\n    \"\"\"\n    total_rows = show_shape(dataframe)[0]\n    return sum(\n        dataframe[col].nunique() / total_rows &gt;= threshold for col in dataframe.columns\n    )\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na","title":"<code>count_rows_with_all_na(dataframe)</code>","text":"<p>Count the number of rows in the DataFrame where all values are missing (NaN).</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_all_na--returns","title":"Returns","text":"<p>int     The number of rows where every column is NaN.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_rows_with_all_na(dataframe):\n    \"\"\"\n    Count the number of rows in the DataFrame where all values are missing (NaN).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of rows where every column is NaN.\n\n    \"\"\"\n\n    return dataframe.isna().all(axis=1).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na","title":"<code>count_rows_with_any_na(dataframe)</code>","text":"<p>Count the number of rows in the DataFrame that contain at least one missing (NaN) value.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_rows_with_any_na--returns","title":"Returns","text":"<p>int     The number of rows with at least one NaN value.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_rows_with_any_na(dataframe):\n    \"\"\"\n    Count the number of rows in the DataFrame that contain at least one missing (NaN) value.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The number of rows with at least one NaN value.\n\n    \"\"\"\n\n    return dataframe.isna().any(axis=1).sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_total_na","title":"<code>count_total_na(dataframe)</code>","text":"<p>Calculate the total number of missing (NaN) values in the entire DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_total_na--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.count_total_na--returns","title":"Returns","text":"<p>int     The total count of NaN values in the DataFrame.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_total_na(dataframe):\n    \"\"\"\n    Calculate the total number of missing (NaN) values in the entire DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The total count of NaN values in the DataFrame.\n\n    \"\"\"\n\n    return dataframe.isna().sum().sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.count_unique_values","title":"<code>count_unique_values(dataframe, columns, n_modes=2, dropna=False, ascending=False)</code>","text":"<p>For each specified column, report:   - unique_count: total unique entries (incl. NaN if dropna=False)   - top_modes: list of the top <code>n_modes</code> (value, count) tuples.</p>"},{"location":"api/#jcds.eda.inspect.count_unique_values--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame columns : str or list of str     Column name or list of column names to analyze. n_modes : int, optional     How many top values (modes) to return per column. Default is 2. dropna : bool, optional     Whether to exclude NaNs from counts and mode ranking. Default is False.</p>"},{"location":"api/#jcds.eda.inspect.count_unique_values--returns","title":"Returns","text":"<p>dict     Mapping each column to a dict with keys:       - \"unique_count\": int       - \"top_modes\": list of (value, count) tuples</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def count_unique_values(dataframe, columns, n_modes=2, dropna=False, ascending=False):\n    \"\"\"\n    For each specified column, report:\n      - unique_count: total unique entries (incl. NaN if dropna=False)\n      - top_modes: list of the top `n_modes` (value, count) tuples.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n    columns : str or list of str\n        Column name or list of column names to analyze.\n    n_modes : int, optional\n        How many top values (modes) to return per column. Default is 2.\n    dropna : bool, optional\n        Whether to exclude NaNs from counts and mode ranking. Default is False.\n\n    Returns\n    -------\n    dict\n        Mapping each column to a dict with keys:\n          - \"unique_count\": int\n          - \"top_modes\": list of (value, count) tuples\n    \"\"\"\n    if isinstance(columns, str):\n        cols = [columns]\n    else:\n        cols = list(columns)\n\n    result = {}\n    for col in cols:\n        if col not in dataframe.columns:\n            raise KeyError(f\"Column not found: {col}\")\n        ucount = dataframe[col].nunique(dropna=dropna)\n        vcount = dataframe[col].value_counts(dropna=dropna)\n        top_modes = list(vcount.head(n_modes).items())\n        result[col] = {\"unique_count\": ucount, \"top_modes\": top_modes}\n    return result\n</code></pre>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary","title":"<code>get_dtype_summary(dataframe)</code>","text":"<p>Returns a dictionary summarizing the count of common data types.</p>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.get_dtype_summary--returns","title":"Returns","text":"<p>dict     Keys are data types (as strings), values are column counts.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def get_dtype_summary(dataframe):\n    \"\"\"\n    Returns a dictionary summarizing the count of common data types.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n\n    Returns\n    -------\n    dict\n        Keys are data types (as strings), values are column counts.\n    \"\"\"\n    type_counts = {\n        \"object\": 0,\n        \"int\": 0,\n        \"float\": 0,\n        \"bool\": 0,\n        \"category\": 0,\n        \"datetime\": 0,\n        \"other\": 0,\n    }\n\n    for col in dataframe.columns:\n        dtype = dataframe[col].dtype\n\n        if ptypes.is_bool_dtype(dtype):\n            type_counts[\"bool\"] += 1\n        elif ptypes.is_integer_dtype(dtype):\n            type_counts[\"int\"] += 1\n        elif ptypes.is_float_dtype(dtype):\n            type_counts[\"float\"] += 1\n        elif isinstance(dtype, pd.CategoricalDtype):\n            type_counts[\"category\"] += 1\n        elif ptypes.is_datetime64_any_dtype(dtype):\n            type_counts[\"datetime\"] += 1\n        elif ptypes.is_object_dtype(dtype):\n            type_counts[\"object\"] += 1\n        else:\n            type_counts[\"other\"] += 1\n\n    return type_counts\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_binary_list","title":"<code>show_binary_list(dataframe, dropna=True)</code>","text":"<p>Identify binary columns in a DataFrame, optionally considering missing values.</p>"},{"location":"api/#jcds.eda.inspect.show_binary_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. dropna : bool, optional     If True, NaN values are excluded when determining binary columns. Default is True.</p>"},{"location":"api/#jcds.eda.inspect.show_binary_list--returns","title":"Returns","text":"<p>dict     A dictionary with two keys:         - \"binary_columns\": list of column names that contain exactly two unique non-null values.         - \"binary_with_nan\": list of column names that have two unique non-null values and also include NaNs.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_binary_list(dataframe, dropna=True):\n    \"\"\"\n    Identify binary columns in a DataFrame, optionally considering missing values.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    dropna : bool, optional\n        If True, NaN values are excluded when determining binary columns. Default is True.\n\n    Returns\n    -------\n    dict\n        A dictionary with two keys:\n            - \"binary_columns\": list of column names that contain exactly two unique non-null values.\n            - \"binary_with_nan\": list of column names that have two unique non-null values and also include NaNs.\n\n    \"\"\"\n\n    binary_cols = []\n    binary_with_nan = []\n    for col in dataframe.columns:\n        unique_vals = dataframe[col].unique()\n        unique_vals_no_nan = pd.Series(unique_vals).dropna().unique()\n\n        if len(unique_vals_no_nan) == 2:\n            if pd.isna(unique_vals).any():\n                binary_with_nan.append(col)\n            else:\n                binary_cols.append(col)\n\n    return {\"binary_columns\": binary_cols, \"binary_with_nan\": binary_with_nan}\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_catvar","title":"<code>show_catvar(dataframe)</code>","text":"<p>Identify and return a list of categorical variables in the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_catvar--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_catvar--returns","title":"Returns","text":"<p>list of str     A list of column names that have categorical or object data types.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_catvar(dataframe):\n    \"\"\"\n    Identify and return a list of categorical variables in the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names that have categorical or object data types.\n\n    \"\"\"\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    return cat_features\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_constantvars","title":"<code>show_constantvars(dataframe, verbose=False)</code>","text":"<p>Identify columns with only one unique value (including NaNs).</p>"},{"location":"api/#jcds.eda.inspect.show_constantvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame verbose : bool, optional     Whether to print a summary message (default is False). Returns</p> <p>list of str     Column names that are constant.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_constantvars(dataframe, verbose=False):\n    \"\"\"\n    Identify columns with only one unique value (including NaNs).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n    verbose : bool, optional\n        Whether to print a summary message (default is False).\n    Returns\n    -------\n    list of str\n        Column names that are constant.\n    \"\"\"\n    if verbose:\n        print(\"Columns (only one unique value)\")\n    col_list = []\n    for col in dataframe.columns:\n        if dataframe[col].nunique(dropna=False) == 1:\n            col_list.append(col)\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_convar","title":"<code>show_convar(dataframe)</code>","text":"<p>Identify and return a list of continuous (non-categorical) variables in the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_convar--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_convar--returns","title":"Returns","text":"<p>list of str     A list of column names that are not of categorical or object data types.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_convar(dataframe):\n    \"\"\"\n    Identify and return a list of continuous (non-categorical) variables in the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names that are not of categorical or object data types.\n\n    \"\"\"\n\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    return cont_features\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns","title":"<code>show_datetime_columns(dataframe)</code>","text":"<p>Identify columns with datetime-like data types.</p>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_datetime_columns--returns","title":"Returns","text":"<p>list of str     A list of column names whose dtype is datetime64[ns] or similar.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_datetime_columns(dataframe):\n    \"\"\"\n    Identify columns with datetime-like data types.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n\n    Returns\n    -------\n    list of str\n        A list of column names whose dtype is datetime64[ns] or similar.\n    \"\"\"\n    dt_cols = []\n    for col in dataframe.columns:\n        col_dtype = dataframe[col].dtype\n        if pd.api.types.is_datetime64_any_dtype(col_dtype):\n            dt_cols.append(col)\n    return dt_cols\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_dimensions","title":"<code>show_dimensions(dataframe)</code>","text":"<p>Return structural and memory usage information for the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dimensions--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dimensions--returns","title":"Returns","text":"<p>tuple     A tuple containing:     - rows (int): Number of rows     - cols (int): Number of columns     - size (int): Total number of data cells (rows \u00d7 columns)     - memory_use (float): Approximate memory usage in megabytes (MB)</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_dimensions(dataframe):\n    \"\"\"\n    Return structural and memory usage information for the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - rows (int): Number of rows\n        - cols (int): Number of columns\n        - size (int): Total number of data cells (rows \u00d7 columns)\n        - memory_use (float): Approximate memory usage in megabytes (MB)\n    \"\"\"\n    rows, cols = dataframe.shape\n    size = dataframe.size\n    memory_use = dataframe.memory_usage(deep=True).sum()\n    # convert from bytes to megabytes, round to 2 dec places\n    memory_use = round((memory_use / 1024**2), 2)\n    return rows, cols, size, memory_use\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_dupes","title":"<code>show_dupes(dataframe)</code>","text":"<p>Return the number of duplicate rows in the given DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dupes--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_dupes--returns","title":"Returns","text":"<p>int     The count of duplicated rows in the DataFrame.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_dupes(dataframe):\n    \"\"\"\n    Return the number of duplicate rows in the given DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    int\n        The count of duplicated rows in the DataFrame.\n\n    \"\"\"\n    dupes = dataframe.duplicated()\n    return dupes.sum()\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_highcardvars","title":"<code>show_highcardvars(dataframe, percent_unique=90, verbose=False)</code>","text":"<p>Identify categorical columns with high cardinality (&gt;= percent_unique).</p>"},{"location":"api/#jcds.eda.inspect.show_highcardvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. percent_unique : float, optional     Minimum % of unique values (vs. total rows) to consider high-cardinality. verbose : bool, optional     Whether to print a summary message (default is False). Returns</p> <p>list of tuples     List of (column name, percent unique) tuples.</p> <p>Docstring generated with assistance from ChatGPT.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_highcardvars(dataframe, percent_unique=90, verbose=False):\n    \"\"\"\n    Identify categorical columns with high cardinality (&gt;= percent_unique).\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    percent_unique : float, optional\n        Minimum % of unique values (vs. total rows) to consider high-cardinality.\n    verbose : bool, optional\n        Whether to print a summary message (default is False).\n    Returns\n    -------\n    list of tuples\n        List of (column name, percent unique) tuples.\n\n    Docstring generated with assistance from ChatGPT.\n    \"\"\"\n    if verbose:\n        print(f\"Cateogrical variables with cardinality &gt;= {percent_unique}%\")\n\n    col_list = []\n    total_rows = show_shape(dataframe)[0]\n    cat_cols = show_catvar(dataframe)\n    for col in cat_cols:\n        count = dataframe[col].nunique()\n        percent = (count / total_rows) * 100\n        if percent &gt;= percent_unique:\n            col_list.append((col, percent))\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_lowcardvars","title":"<code>show_lowcardvars(dataframe, max_unique=10, verbose=False)</code>","text":"<p>Return a list of categorical variables with unique values less than or equal to the specified threshold.</p>"},{"location":"api/#jcds.eda.inspect.show_lowcardvars--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. max_unique : int, optional     The maximum number of unique values allowed for a variable to be considered low cardinality. Default is 10. verbose : bool, optional     Whether to print a summary message (default is False). Returns</p> <p>list of tuple     A list of tuples where each tuple contains the column name and the number of unique values.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_lowcardvars(dataframe, max_unique=10, verbose=False):\n    \"\"\"\n    Return a list of categorical variables with unique values less than or equal to the specified threshold.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    max_unique : int, optional\n        The maximum number of unique values allowed for a variable to be considered low cardinality. Default is 10.\n    verbose : bool, optional\n        Whether to print a summary message (default is False).\n    Returns\n    -------\n    list of tuple\n        A list of tuples where each tuple contains the column name and the number of unique values.\n\n    \"\"\"\n    if verbose:\n        print(f\"Categorical variables with cardinality &lt;= {max_unique}\")\n    col_list = []\n    cols = show_catvar(dataframe)\n    for col in cols:\n        count = dataframe[col].nunique()\n        if count &lt;= max_unique:\n            col_list.append((col, count))\n    return col_list\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_memory_use","title":"<code>show_memory_use(dataframe)</code>","text":"<p>Returns memory usage of the dataframe in megabytes (MB)</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_memory_use(dataframe):\n    \"\"\"\n    Returns memory usage of the dataframe in megabytes (MB)\n    \"\"\"\n    memory_usage = dataframe.memory_usage(deep=True).sum()\n    # convert from bytes to megabytes\n    memory_usage = memory_usage / 1024**2\n    return float(memory_usage)\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_missing_summary","title":"<code>show_missing_summary(dataframe, sort=True, threshold=0.0)</code>","text":"<p>Summarize missing values in the DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_missing_summary--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. sort : bool, optional     If True, sorts the result by descending missing count. Default is True. threshold : float, optional     Minimum percentage (0\u2013100) of missing values to include a column. Default is 0.0.</p>"},{"location":"api/#jcds.eda.inspect.show_missing_summary--returns","title":"Returns","text":"<p>dict     A dictionary where keys are column names and values are tuples of     (missing count, percent missing), filtered by the threshold.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_missing_summary(dataframe, sort=True, threshold=0.0):\n    \"\"\"\n    Summarize missing values in the DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    sort : bool, optional\n        If True, sorts the result by descending missing count. Default is True.\n    threshold : float, optional\n        Minimum percentage (0\u2013100) of missing values to include a column. Default is 0.0.\n\n    Returns\n    -------\n    dict\n        A dictionary where keys are column names and values are tuples of\n        (missing count, percent missing), filtered by the threshold.\n    \"\"\"\n    null_counts = dataframe.isnull().sum()\n    null_counts = null_counts[null_counts &gt; 0]\n    total_rows = len(dataframe)\n\n    summary = {}\n    for col, count in null_counts.items():\n        pct = (count / total_rows) * 100\n        if pct &gt;= threshold:\n            summary[col] = (int(count), round(pct, 1))\n\n    if sort:\n        summary = dict(sorted(summary.items(), key=lambda x: x[1][0], reverse=True))\n\n    return summary\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_nearconstvars","title":"<code>show_nearconstvars(dataframe, threshold=0.95, verbose=False)</code>","text":"<p>Finds columns where a single value makes up more than <code>threshold</code> proportion of the data.</p>"},{"location":"api/#jcds.eda.inspect.show_nearconstvars--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataframe. threshold : float, optional     The proportion above which a column is considered near-constant (default is 0.95). verbose : bool, optional     Whether to print a summary message (default is False).</p>"},{"location":"api/#jcds.eda.inspect.show_nearconstvars--returns","title":"Returns","text":"<p>List[str]     List of column names that are near-constant.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_nearconstvars(dataframe, threshold=0.95, verbose=False):\n    \"\"\"\n    Finds columns where a single value makes up more than `threshold` proportion of the data.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input dataframe.\n    threshold : float, optional\n        The proportion above which a column is considered near-constant (default is 0.95).\n    verbose : bool, optional\n        Whether to print a summary message (default is False).\n\n    Returns\n    -------\n    List[str]\n        List of column names that are near-constant.\n    \"\"\"\n    if verbose:\n        print(f\"Columns with cardinality &lt;= {threshold*100:.1f}% \")\n\n    near_constant = []\n    for col in dataframe.columns:\n        top_freq = dataframe[col].value_counts(normalize=True, dropna=False).values[0]\n        if top_freq &gt;= threshold:\n            near_constant.append(col)\n\n    return near_constant\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns","title":"<code>show_possible_datetime_columns(dataframe, sample_size=5)</code>","text":"<p>Identify object columns that may contain datetime-like strings.</p>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. sample_size : int     Number of values to sample for date parsing test.</p>"},{"location":"api/#jcds.eda.inspect.show_possible_datetime_columns--returns","title":"Returns","text":"<p>list of str     Columns that appear to contain datetime-like strings.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_possible_datetime_columns(dataframe, sample_size=5):\n    \"\"\"\n    Identify object columns that may contain datetime-like strings.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    sample_size : int\n        Number of values to sample for date parsing test.\n\n    Returns\n    -------\n    list of str\n        Columns that appear to contain datetime-like strings.\n    \"\"\"\n    possible_date_cols = []\n\n    for col in dataframe.select_dtypes(include=\"object\").columns:\n        sample_values = dataframe[col].dropna().head(sample_size)\n        parse_attempts = sample_values.apply(\n            lambda x: pd.to_datetime(x, errors=\"coerce\")\n        )\n        success_ratio = parse_attempts.notna().mean()\n        if success_ratio &gt;= 0.8:  # 80% of values parse as dates\n            possible_date_cols.append(col)\n\n    return possible_date_cols\n</code></pre>"},{"location":"api/#jcds.eda.inspect.show_shape","title":"<code>show_shape(dataframe)</code>","text":"<p>Return the shape (number of rows and columns) of the given DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_shape--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame.</p>"},{"location":"api/#jcds.eda.inspect.show_shape--returns","title":"Returns","text":"<p>tuple of int     A tuple containing the number of rows and columns in the DataFrame.</p> Source code in <code>src/jcds/eda/inspect.py</code> <pre><code>def show_shape(dataframe):\n    \"\"\"\n    Return the shape (number of rows and columns) of the given DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n\n    Returns\n    -------\n    tuple of int\n        A tuple containing the number of rows and columns in the DataFrame.\n\n    \"\"\"\n    return dataframe.shape\n</code></pre>"},{"location":"api/#eda-outliers","title":"EDA Outliers","text":""},{"location":"api/#jcds.eda.outliers.detect_outliers_iqr","title":"<code>detect_outliers_iqr(dataframe, threshold=1.5, return_mask=False)</code>","text":"<p>Detect outliers in numeric (non-binary) columns using the IQR method.</p>"},{"location":"api/#jcds.eda.outliers.detect_outliers_iqr--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame. threshold : float, optional     The IQR multiplier to determine outlier bounds. Default is 1.5. return_mask : bool, optional     If True, returns a boolean mask DataFrame. If False, returns outlier counts.</p>"},{"location":"api/#jcds.eda.outliers.detect_outliers_iqr--returns","title":"Returns","text":"<p>dict or pd.DataFrame     If return_mask is False: a dict {column: count of outliers}     If return_mask is True: a DataFrame with boolean values (True = outlier)</p> Source code in <code>src/jcds/eda/outliers.py</code> <pre><code>def detect_outliers_iqr(dataframe, threshold=1.5, return_mask=False):\n    \"\"\"\n    Detect outliers in numeric (non-binary) columns using the IQR method.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame.\n    threshold : float, optional\n        The IQR multiplier to determine outlier bounds. Default is 1.5.\n    return_mask : bool, optional\n        If True, returns a boolean mask DataFrame. If False, returns outlier counts.\n\n    Returns\n    -------\n    dict or pd.DataFrame\n        If return_mask is False: a dict {column: count of outliers}\n        If return_mask is True: a DataFrame with boolean values (True = outlier)\n    \"\"\"\n    numeric_cols = show_convar(dataframe)\n    binary_info = show_binary_list(dataframe)\n    binary_cols = binary_info[\"binary_columns\"] + binary_info[\"binary_with_nan\"]\n\n    # filter out binary cols\n    outlier_cols = []\n    for col in numeric_cols:\n        if col not in binary_cols:\n            outlier_cols.append(col)\n\n    outlier_counts = {}\n    outlier_mask = pd.DataFrame(False, index=dataframe.index, columns=dataframe.columns)\n\n    for col in outlier_cols:\n        Q1 = dataframe[col].quantile(0.25)\n        Q3 = dataframe[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower = Q1 - threshold * IQR\n        upper = Q3 + threshold * IQR\n\n        is_outlier = (dataframe[col] &lt; lower) | (dataframe[col] &gt; upper)\n        outlier_mask[col] = is_outlier\n\n        if not return_mask:\n            outlier_counts[col] = int(is_outlier.sum())\n\n    return outlier_mask if return_mask else outlier_counts\n</code></pre>"},{"location":"api/#column-utilities","title":"Column Utilities","text":""},{"location":"api/#jcds.eda.lists.get_cat_list","title":"<code>get_cat_list(dataframe)</code>","text":"<p>Return a list of categorical column names or the subset of the DataFrame containing only categorical columns.</p>"},{"location":"api/#jcds.eda.lists.get_cat_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to analyze. return_names : bool     If True, returns a list of column names. If False, returns a DataFrame slice.</p>"},{"location":"api/#jcds.eda.lists.get_cat_list--returns","title":"Returns","text":"<p>list of str or pd.DataFrame     Categorical column names if <code>return_names</code> is True, or a DataFrame containing only categorical columns if False.</p> Source code in <code>src/jcds/eda/lists.py</code> <pre><code>@deprecated(\n        reason=\"This is being replaced by show_catvar()\",\n        version=\"0.3.0\"\n)\ndef get_cat_list(dataframe):\n    \"\"\"\n    Return a list of categorical column names or the subset of the DataFrame containing only categorical columns.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to analyze.\n    return_names : bool\n        If True, returns a list of column names. If False, returns a DataFrame slice.\n\n    Returns\n    -------\n    list of str or pd.DataFrame\n        Categorical column names if `return_names` is True, or a DataFrame containing only categorical columns if False.\n\n    \"\"\"\n\n    cat_list = dataframe.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n    return cat_list\n</code></pre>"},{"location":"api/#jcds.eda.lists.get_cont_list","title":"<code>get_cont_list(dataframe)</code>","text":"<p>Return a list of continuous (non-categorical) column names or the subset of the DataFrame containing only continuous columns.</p>"},{"location":"api/#jcds.eda.lists.get_cont_list--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to analyze. return_names : bool     If True, returns a list of column names. If False, returns a DataFrame slice.</p>"},{"location":"api/#jcds.eda.lists.get_cont_list--returns","title":"Returns","text":"<p>list of str or pd.DataFrame     Continuous column names if <code>return_names</code> is True, or a DataFrame containing only continuous columns if False.</p> Source code in <code>src/jcds/eda/lists.py</code> <pre><code>@deprecated(\n        reason=\"This is being replaced by show_contvar()\", \n        version=\"0.3.0\"\n)\ndef get_cont_list(dataframe):\n    \"\"\"\n    Return a list of continuous (non-categorical) column names or the subset of the DataFrame containing only continuous columns.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to analyze.\n    return_names : bool\n        If True, returns a list of column names. If False, returns a DataFrame slice.\n\n    Returns\n    -------\n    list of str or pd.DataFrame\n        Continuous column names if `return_names` is True, or a DataFrame containing only continuous columns if False.\n\n    \"\"\"\n\n    cont_list = dataframe.select_dtypes(exclude=[\"category\", \"object\"]).columns.tolist()\n    return cont_list\n</code></pre>"},{"location":"api/#jcds.eda.lists.list_unique_values","title":"<code>list_unique_values(dataframe, column)</code>","text":"<p>Print the unique values in one or more specified columns of a DataFrame and display the corresponding code snippet.</p>"},{"location":"api/#jcds.eda.lists.list_unique_values--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input pandas DataFrame. column : str or list of str     The column name or list of column names to inspect for unique values.</p>"},{"location":"api/#jcds.eda.lists.list_unique_values--returns","title":"Returns","text":"<p>None     This function prints output to the console and does not return a value.</p> Source code in <code>src/jcds/eda/lists.py</code> <pre><code>def list_unique_values(dataframe, column):\n    \"\"\"\n    Print the unique values in one or more specified columns of a DataFrame and display the corresponding code snippet.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input pandas DataFrame.\n    column : str or list of str\n        The column name or list of column names to inspect for unique values.\n\n    Returns\n    -------\n    None\n        This function prints output to the console and does not return a value.\n\n    \"\"\"\n    display_code = f'DataFrame[\"{column}\"].unique().tolist()'\n    print_code_line(display_code)\n\n    if isinstance(column, list):\n        for col in column:\n            print(f\"Unique values in '{col}':\")\n            print(dataframe[col].unique().tolist())\n            print(\"---\")\n    else:\n        print(f\"Unique values in '{column}':\")\n        print(dataframe[column].unique().tolist())\n</code></pre>"},{"location":"api/#datetime-utilities","title":"Datetime Utilities","text":""},{"location":"api/#jcds.eda.datetime.create_dt_col","title":"<code>create_dt_col(dataframe, datetime_col, col_type='month')</code>","text":"<p>Wrapper for create_dt_cols that creates a single datetime-derived column.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_col--parameters","title":"Parameters","text":"<p>dataframe : DataFrame     The input DataFrame. datetime_col : str     Name of the column containing datetime values. col_type : str     A single datetime component to extract.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_col--returns","title":"Returns","text":"<p>DataFrame     A copy of the DataFrame with one new datetime feature column added.</p> Source code in <code>src/jcds/eda/datetime.py</code> <pre><code>def create_dt_col(dataframe, datetime_col, col_type=\"month\"):\n    \"\"\"\n    Wrapper for create_dt_cols that creates a single datetime-derived column.\n\n    Parameters\n    ----------\n    dataframe : DataFrame\n        The input DataFrame.\n    datetime_col : str\n        Name of the column containing datetime values.\n    col_type : str\n        A single datetime component to extract.\n\n    Returns\n    -------\n    DataFrame\n        A copy of the DataFrame with one new datetime feature column added.\n\n    \"\"\"\n    return create_dt_cols(dataframe, datetime_col, [col_type])\n</code></pre>"},{"location":"api/#jcds.eda.datetime.create_dt_cols","title":"<code>create_dt_cols(dataframe, datetime_col, col_types=['month'])</code>","text":"<p>Add one or more datetime-derived columns to a DataFrame from a single datetime column.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--parameters","title":"Parameters","text":"<p>dataframe : DataFrame     The input DataFrame. datetime_col : str     Name of the column containing datetime values. col_types : str or list of str, default [\"month\"]     One or more datetime components to extract. Supported values:     \"year\", \"month\", \"day\", \"weekday\", \"weekday_name\", \"weekofyear\",     \"quarter\", \"is_weekend\", \"dayofyear\", \"is_month_start\", \"is_month_end\".</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--returns","title":"Returns","text":"<p>DataFrame     A copy of the DataFrame with the new datetime feature columns added.</p>"},{"location":"api/#jcds.eda.datetime.create_dt_cols--raises","title":"Raises","text":"<p>ValueError     If the datetime_col is missing or if any component in col_types is unsupported.</p> Source code in <code>src/jcds/eda/datetime.py</code> <pre><code>def create_dt_cols(dataframe, datetime_col, col_types=[\"month\"]):\n    \"\"\"\n    Add one or more datetime-derived columns to a DataFrame from a single datetime column.\n\n    Parameters\n    ----------\n    dataframe : DataFrame\n        The input DataFrame.\n    datetime_col : str\n        Name of the column containing datetime values.\n    col_types : str or list of str, default [\"month\"]\n        One or more datetime components to extract. Supported values:\n        \"year\", \"month\", \"day\", \"weekday\", \"weekday_name\", \"weekofyear\",\n        \"quarter\", \"is_weekend\", \"dayofyear\", \"is_month_start\", \"is_month_end\".\n\n    Returns\n    -------\n    DataFrame\n        A copy of the DataFrame with the new datetime feature columns added.\n\n    Raises\n    ------\n    ValueError\n        If the datetime_col is missing or if any component in col_types is unsupported.\n\n    \"\"\"\n    supported_types = {\n        \"year\": lambda x: x.dt.year,\n        \"month\": lambda x: x.dt.month,\n        \"day\": lambda x: x.dt.day,\n        \"weekday\": lambda x: x.dt.weekday,\n        \"weekday_name\": lambda x: x.dt.day_name(),\n        \"weekofyear\": lambda x: x.dt.isocalendar().week,\n        \"quarter\": lambda x: x.dt.quarter,\n        \"is_weekend\": lambda x: x.dt.weekday &gt;= 5,\n        \"dayofyear\": lambda x: x.dt.dayofyear,\n        \"is_month_start\": lambda x: x.dt.is_month_start,\n        \"is_month_end\": lambda x: x.dt.is_month_end,\n    }\n\n    if datetime_col not in dataframe.columns:\n        raise ValueError(f\"Column '{datetime_col}' not found in DataFrame.\")\n\n    if isinstance(col_types, str):\n        col_types = [col_types]\n\n    unsupported = [ct for ct in col_types if ct not in supported_types]\n    if unsupported:\n        raise ValueError(\n            f\"Unsupported col_type(s): {unsupported}. Must be one of: {list(supported_types)}\"\n        )\n\n    dataframe = dataframe.copy()\n\n    if not pd.api.types.is_datetime64_any_dtype(dataframe[datetime_col]):\n        # Check for consistent format\n        sample = dataframe[datetime_col].dropna().astype(str)\n        has_dash = sample.str.contains(\"-\").any()\n        has_slash = sample.str.contains(\"/\").any()\n\n        if has_dash and has_slash:\n            raise ValueError(\n                f\"Inconsistent datetime format detected in column '{datetime_col}'. \"\n                \"Mix of '-' and '/' found. Please standardize format before applying datetime expansion.\"\n            )\n\n        try:\n            dataframe[datetime_col] = pd.to_datetime(dataframe[datetime_col])\n        except Exception as e:\n            raise ValueError(f\"Could not convert '{datetime_col}' to datetime: {e}\")\n\n    for col_type in col_types:\n        new_col = f\"{datetime_col}_{col_type}\"\n        dataframe[new_col] = supported_types[col_type](dataframe[datetime_col])\n\n    return dataframe\n</code></pre>"},{"location":"api/#quick-reports","title":"Quick Reports","text":""},{"location":"api/#jcds.eda.reports.display_all_col_head","title":"<code>display_all_col_head(dataframe, head=5)</code>","text":"<p>Display the first few rows of a DataFrame with all columns visible.</p> <p>Temporarily adjusts the pandas display settings to ensure all columns are shown, regardless of the total number, and uses IPython's <code>display()</code> function for clean notebook output.</p>"},{"location":"api/#jcds.eda.reports.display_all_col_head--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to display. head : int, optional     The number of rows to show from the top of the DataFrame. Defaults to 5.</p>"},{"location":"api/#jcds.eda.reports.display_all_col_head--notes","title":"Notes","text":"<ul> <li>Temporarily sets the pandas display option to show all columns.</li> <li>Uses IPython's <code>display()</code> function for cleaner notebook output.</li> </ul>"},{"location":"api/#jcds.eda.reports.display_all_col_head--returns","title":"Returns","text":"<p>None     This function prints to the notebook interface and does not return a value.</p> Source code in <code>src/jcds/eda/reports.py</code> <pre><code>def display_all_col_head(dataframe, head=5):\n    \"\"\"\n    Display the first few rows of a DataFrame with all columns visible.\n\n    Temporarily adjusts the pandas display settings to ensure all columns are shown, regardless of the total number,\n    and uses IPython's `display()` function for clean notebook output.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to display.\n    head : int, optional\n        The number of rows to show from the top of the DataFrame. Defaults to 5.\n\n    Notes\n    -----\n    - Temporarily sets the pandas display option to show all columns.\n    - Uses IPython's `display()` function for cleaner notebook output.\n\n    Returns\n    -------\n    None\n        This function prints to the notebook interface and does not return a value.\n\n    \"\"\"\n\n    with pd.option_context(\"display.max_columns\", None):\n        display(dataframe.head(head))\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.dqr_cat","title":"<code>dqr_cat(dataframe)</code>","text":"<p>Generate a data quality report for categorical features in a given DataFrame.</p> <p>This function calculates and prints the following metrics for each feature in the provided list: - Total count of non-missing values - Total count of missing values - Percentage of missing values - Cardinality (number of unique values) - Mode 1 (most frequent value), its frequency, and percentage - Mode 2 (second most frequent value), its frequency, and percentage - Descriptive statistics for each feature</p>"},{"location":"api/#jcds.eda.reports.dqr_cat--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The DataFrame containing the data to be analyzed. list_of_features : list of str     The list of column names (features) for which the data quality report is generated.</p>"},{"location":"api/#jcds.eda.reports.dqr_cat--returns","title":"Returns","text":"<p>None     This function prints the data quality report to the console.</p> Source code in <code>src/jcds/eda/reports.py</code> <pre><code>def dqr_cat(dataframe):\n    \"\"\"\n    Generate a data quality report for categorical features in a given DataFrame.\n\n    This function calculates and prints the following metrics for each feature in the provided list:\n    - Total count of non-missing values\n    - Total count of missing values\n    - Percentage of missing values\n    - Cardinality (number of unique values)\n    - Mode 1 (most frequent value), its frequency, and percentage\n    - Mode 2 (second most frequent value), its frequency, and percentage\n    - Descriptive statistics for each feature\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The DataFrame containing the data to be analyzed.\n    list_of_features : list of str\n        The list of column names (features) for which the data quality report is generated.\n\n    Returns\n    -------\n    None\n        This function prints the data quality report to the console.\n\n    \"\"\"\n\n    # Initialize variables\n    round_to = 2\n    list_feature_name = []\n    list_count = []\n    list_missing = []\n    list_percent = []\n    list_cardinality = []\n    list_mode1 = []\n    list_mode1_freq = []\n    list_mode1_perc = []\n    list_mode2 = []\n    list_mode2_freq = []\n    list_mode2_perc = []\n\n    # Create list of non-categorical values\n    list_of_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n\n    # Total rows\n    total_rows = dataframe.shape[0]\n\n    if len(list_of_features) == 0:\n        print(\"This dataset does not have any categorical columns.\")\n        return\n\n    print(\"The categorical features are: \")\n    print(list_of_features)\n\n    for feature in list_of_features:\n\n        total_count = dataframe[feature].count()\n        total_missing = dataframe[feature].isnull().sum()\n        percent_missing = np.round(total_missing / total_rows * 100, round_to)\n        cardinality = len(dataframe[feature].unique())\n\n        # Use value counts to get modes\n        results = dataframe[feature].value_counts()\n        # Calculate mode\n        mode1_name = results.index[0]\n        mode1_count = results.iloc[0]\n        mode1_percent = np.round((mode1_count / total_count) * 100, round_to)\n\n        # Initialize mode 2 variables\n        mode2_name = None\n        mode2_count = 0\n        mode2_percent = 0.0\n\n        # Calculate 2nd mode if it exists\n        if len(results) &gt; 1:\n            mode2_name = results.index[1]\n            mode2_count = results.iloc[1]\n            mode2_percent = np.round((mode2_count / total_count) * 100, round_to)\n\n        # Append results to lists\n        list_feature_name.append(feature)\n        list_count.append(total_count)\n        list_missing.append(total_missing)\n        list_percent.append(percent_missing)\n        list_cardinality.append(cardinality)\n        list_mode1.append(mode1_name)\n        list_mode1_freq.append(mode1_count)\n        list_mode1_perc.append(mode1_percent)\n        list_mode2.append(mode2_name)\n        list_mode2_freq.append(mode2_count)\n        list_mode2_perc.append(mode2_percent)\n\n    # Create dataframes\n    data = {\n        \"Feature\": list_feature_name,\n        \"Count\": list_count,\n        \"Missing\": list_missing,\n        \"% Missing\": list_percent,\n        \"Cardinality\": list_cardinality,\n    }\n\n    data_mode1 = {\n        \"Feature\": list_feature_name,\n        \"Mode 1\": list_mode1,\n        \"Mode 1 Freq.\": list_mode1_freq,\n        \"Mode 1 %\": list_mode1_perc,\n    }\n\n    data_mode2 = {\n        \"Feature\": list_feature_name,\n        \"Mode 2\": list_mode2,\n        \"Mode 2 Freq.\": list_mode2_freq,\n        \"Mode 2 %\": list_mode2_perc,\n    }\n\n    df = pd.DataFrame(data)\n    df1 = pd.DataFrame(data_mode1)\n    df2 = pd.DataFrame(data_mode2)\n\n    # Get descriptive statistics and transpose\n    stats = dataframe[list_of_features].describe(include=\"object\")\n    transposed_stats = stats.T\n\n    # Print results\n    print(\"Data Quality Report for Categorical Features\")\n    print(f\"Total features: {len(list_of_features)} / {total_rows} rows\")\n    print(\"============================================\")\n    print(\"Stats\")\n    print(\"-----\")\n    display(df)\n\n    print(\"\\n\")\n    print(\"Mode 1\")\n    print(\"------\")\n    display(df1)\n\n    print(\"\\n\")\n    print(\"Mode 2\")\n    print(\"------\")\n    display(df2)\n\n    print(\"\\n\")\n    print(\"Descriptive Stats\")\n    print(\"-----------------\")\n    display(transposed_stats)\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.dqr_cont","title":"<code>dqr_cont(dataframe)</code>","text":"<p>Generate a data quality report for continuous features in a given DataFrame.</p> <p>This function calculates and prints the following metrics for each feature in the provided list: - Total count of non-missing values - Total count of missing values - Percentage of missing values - Cardinality (number of unique values) - Descriptive statistics (mean, standard deviation, min, max)</p>"},{"location":"api/#jcds.eda.reports.dqr_cont--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The DataFrame containing the data to be analyzed. list_of_features : list of str     The list of column names (features) for which the data quality report is generated.</p>"},{"location":"api/#jcds.eda.reports.dqr_cont--returns","title":"Returns","text":"<p>None     This function prints the data quality report to the console.</p> Source code in <code>src/jcds/eda/reports.py</code> <pre><code>def dqr_cont(dataframe):\n    \"\"\"\n    Generate a data quality report for continuous features in a given DataFrame.\n\n    This function calculates and prints the following metrics for each feature in the provided list:\n    - Total count of non-missing values\n    - Total count of missing values\n    - Percentage of missing values\n    - Cardinality (number of unique values)\n    - Descriptive statistics (mean, standard deviation, min, max)\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The DataFrame containing the data to be analyzed.\n    list_of_features : list of str\n        The list of column names (features) for which the data quality report is generated.\n\n    Returns\n    -------\n    None\n        This function prints the data quality report to the console.\n\n    \"\"\"\n\n    # Initialize variables\n    round_to = 2\n    list_feature_name = []\n    list_count = []\n    list_missing = []\n    list_percent = []\n    list_cardinality = []\n\n    # Create list of non-categorical values\n    list_of_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n\n    # Total rows\n    total_rows = dataframe.shape[0]\n\n    if len(list_of_features) == 0:\n        print(\"This dataset does not have any non-categorical features.\")\n        return\n\n    print(\"The non-categorical features are: \")\n    print(list_of_features)\n\n    for feature in list_of_features:\n        # Get stats for each feature\n        total_count = dataframe[feature].count()\n        total_missing = dataframe[feature].isnull().sum()\n        percent_missing = total_missing / total_rows * 100\n        cardinality = len(dataframe[feature].unique())\n\n        # Append result to variables\n        list_feature_name.append(feature)\n        list_count.append(total_count)\n        list_missing.append(total_missing)\n        list_percent.append(np.round(percent_missing, round_to))\n        list_cardinality.append(cardinality)\n\n    # Create dataframe\n    data = {\n        \"Feature\": list_feature_name,\n        \"Count\": list_count,\n        \"Missing\": list_missing,\n        \"% missing\": list_percent,\n        \"Cardinality\": list_cardinality,\n    }\n    df = pd.DataFrame(data)\n\n    # Get descriptive statistics and transpose\n    stats = np.round(dataframe[list_of_features].describe(), round_to)\n    transposed_stats = stats.T\n\n    # Print results\n    print(\"Data Quality for Continous Features\")\n    print(f\"Total Features: {len(list_of_features)} / {total_rows} rows\")\n    display(df)\n\n    print(\"\\n\")\n    print(\"Descriptive Stats\")\n    display(transposed_stats)\n\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.long_report","title":"<code>long_report(dataframe)</code>","text":"<p>Generate a detailed summary report of a pandas DataFrame, including shape, missing value statistics, and a breakdown of categorical and continuous features with their unique value counts.</p> <p>This function prints: - Total number of columns and rows - Number and percentage of rows that are entirely missing - Number and percentage of columns with any missing values - Total number of missing values in the dataset - Count of categorical and continuous features - For each categorical and continuous feature: number of unique values</p>"},{"location":"api/#jcds.eda.reports.long_report--notes","title":"Notes","text":"<ul> <li>Categorical features are detected based on 'object' and 'category' dtypes.</li> <li>Continuous features include all other non-categorical dtypes.</li> <li>Missing value percentages are rounded to two decimal places.</li> <li>A placeholder for <code>.info(memory_usage='deep')</code> is printed but not executed.</li> </ul>"},{"location":"api/#jcds.eda.reports.long_report--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame to analyze.</p>"},{"location":"api/#jcds.eda.reports.long_report--returns","title":"Returns","text":"<p>None     This function prints the detailed summary report to the console.</p> Source code in <code>src/jcds/eda/reports.py</code> <pre><code>@deprecated(\n    reason=\"This will be replaced with data_info(), data_cardinality(), and data_quality().\",\n    version=\"0.3.0\",\n)\ndef long_report(dataframe):\n    \"\"\"\n    Generate a detailed summary report of a pandas DataFrame, including shape, missing value statistics,\n    and a breakdown of categorical and continuous features with their unique value counts.\n\n    This function prints:\n    - Total number of columns and rows\n    - Number and percentage of rows that are entirely missing\n    - Number and percentage of columns with any missing values\n    - Total number of missing values in the dataset\n    - Count of categorical and continuous features\n    - For each categorical and continuous feature: number of unique values\n\n    Notes\n    -----\n    - Categorical features are detected based on 'object' and 'category' dtypes.\n    - Continuous features include all other non-categorical dtypes.\n    - Missing value percentages are rounded to two decimal places.\n    - A placeholder for `.info(memory_usage='deep')` is printed but not executed.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze.\n\n    Returns\n    -------\n    None\n        This function prints the detailed summary report to the console.\n\n    \"\"\"\n\n    ROUND = 2\n    # Get features not labeled as categorical\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get features labeled as categorical\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get shape, total rows and cols\n    total_rows = dataframe.shape[0]\n    total_cols = dataframe.shape[1]\n    # Count/% rows that are missing values\n    rows_with_all_na = dataframe.isna().all(axis=1).sum()\n    percent_na_rows = np.round(rows_with_all_na / total_rows * 100, ROUND)\n    # Count/% cols that are missing values\n    cols_with_na = dataframe.isna().any(axis=0).sum()\n    percent_na_cols = np.round(cols_with_na / total_cols * 100, ROUND)\n    total_na = dataframe.isna().sum().sum()\n\n    print(\"============================================\")\n    print(\"Quick Report - info(memory_usage='deep')\")\n    print(f\"Total cols: {total_cols}\")\n    print(f\"Rows missing all values: {rows_with_all_na} ({percent_na_rows}%)\")\n    print(f\"Total Rows: {total_rows}\")\n    print(f\"Cols with missing values: {cols_with_na} ({percent_na_cols}%)\")\n    print(f\"Total missing values in dataset: {total_na}\")\n    print(\"============================================\")\n    print(f\"Categorical features: {len(cat_features)}\")\n    for cat in cat_features:\n        num_unique = dataframe[cat].unique()\n        print(f\"- {cat}: {len(num_unique)} unique values\")\n    print(\"============================================\")\n    print(f\"Continuous features: {len(cont_features)}\")\n    for cont in cont_features:\n        num_unique = dataframe[cont].unique()\n        print(f\"- {cont}: {len(num_unique)} unique values\")\n\n    # info = dataframe.info(memory_usage='deep')\n    # display(info)\n    return\n</code></pre>"},{"location":"api/#jcds.eda.reports.quick_report","title":"<code>quick_report(dataframe)</code>","text":"<p>Generate a quick summary report of a pandas DataFrame, including shape, missing value statistics, and memory usage.</p> <p>This function prints: - Total number of columns and rows - Number and percentage of rows that are entirely missing - Number and percentage of columns that have any missing values - Total number of missing values in the dataset - Output of <code>DataFrame.info()</code> with memory usage (<code>deep=True</code>)</p>"},{"location":"api/#jcds.eda.reports.quick_report--notes","title":"Notes","text":"<ul> <li>Categorical and continuous feature identification is performed but not displayed.</li> <li>Missing value calculations are rounded to two decimal places.</li> </ul>"},{"location":"api/#jcds.eda.reports.quick_report--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The input DataFrame to analyze.</p>"},{"location":"api/#jcds.eda.reports.quick_report--returns","title":"Returns","text":"<p>None     This function prints the summary report to the console.</p> Source code in <code>src/jcds/eda/reports.py</code> <pre><code>@deprecated(\n    reason=\"This will be replaced with data_info(), data_cardinality(), and data_quality().\",\n    version=\"0.3.0\",\n)\ndef quick_report(dataframe):\n    \"\"\"\n    Generate a quick summary report of a pandas DataFrame, including shape, missing value statistics, and memory usage.\n\n    This function prints:\n    - Total number of columns and rows\n    - Number and percentage of rows that are entirely missing\n    - Number and percentage of columns that have any missing values\n    - Total number of missing values in the dataset\n    - Output of `DataFrame.info()` with memory usage (`deep=True`)\n\n    Notes\n    -----\n    - Categorical and continuous feature identification is performed but not displayed.\n    - Missing value calculations are rounded to two decimal places.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze.\n\n    Returns\n    -------\n    None\n        This function prints the summary report to the console.\n\n    \"\"\"\n\n    ROUND = 2\n    # Get features not labeled as categorical\n    cat_features = dataframe.select_dtypes(\n        include=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get features labeled as categorical\n    cont_features = dataframe.select_dtypes(\n        exclude=[\"category\", \"object\"]\n    ).columns.tolist()\n    # Get shape, total rows and cols\n    total_rows = dataframe.shape[0]\n    total_cols = dataframe.shape[1]\n    # Count/% rows that are missing values\n    rows_with_all_na = dataframe.isna().all(axis=1).sum()\n    percent_na_rows = np.round(rows_with_all_na / total_rows * 100, ROUND)\n    # Count/% cols that are missing values\n    cols_with_na = dataframe.isna().any(axis=0).sum()\n    percent_na_cols = np.round(cols_with_na / total_cols * 100, ROUND)\n    total_na = dataframe.isna().sum().sum()\n\n    print(\"============================================\")\n    print(\"Quick Report - info(memory_usage='deep')\")\n    print(f\"Total cols: {total_cols}\")\n    print(f\"Rows missing all values: {rows_with_all_na} ({percent_na_rows}%)\")\n    print(f\"Total Rows: {total_rows}\")\n    print(f\"Cols with missing values: {cols_with_na} ({percent_na_cols}%)\")\n    print(f\"Total missing values in dataset: {total_na}\")\n    print(\"============================================\")\n</code></pre>"},{"location":"api/#data-inputoutput","title":"Data Input/Output","text":""},{"location":"api/#jcds.dataio.load_csv","title":"<code>load_csv(filepath: Union[str, Path], encodings: Optional[list[str]] = None, preview_bytes: int = 300, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Attempt to load a CSV using multiple encodings. If all fail, preview raw bytes.</p>"},{"location":"api/#jcds.dataio.load_csv--parameters","title":"Parameters","text":"<p>filepath : str or Path     Path to the CSV file. encodings : list of str, optional     List of encodings to try. Defaults to common ones. preview_bytes : int     Number of bytes to show if loading fails (for debugging). **kwargs     Additional arguments passed to <code>pd.read_csv</code>.</p>"},{"location":"api/#jcds.dataio.load_csv--returns","title":"Returns","text":"<p>pd.DataFrame     Loaded DataFrame.</p>"},{"location":"api/#jcds.dataio.load_csv--raises","title":"Raises","text":"<p>UnicodeDecodeError     If no encoding successfully loads the file.</p>"},{"location":"api/#jcds.dataio.load_csv--notes","title":"Notes","text":"Source code in <code>src/jcds/dataio/io_utils.py</code> <pre><code>def load_csv(\n    filepath: Union[str, Path],\n    encodings: Optional[list[str]] = None,\n    preview_bytes: int = 300,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Attempt to load a CSV using multiple encodings. If all fail, preview raw bytes.\n\n    Parameters\n    ----------\n    filepath : str or Path\n        Path to the CSV file.\n    encodings : list of str, optional\n        List of encodings to try. Defaults to common ones.\n    preview_bytes : int\n        Number of bytes to show if loading fails (for debugging).\n    **kwargs\n        Additional arguments passed to `pd.read_csv`.\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded DataFrame.\n\n    Raises\n    ------\n    UnicodeDecodeError\n        If no encoding successfully loads the file.\n\n    Notes\n    -----\n    \"\"\"\n    filepath = Path(filepath)\n    encodings = encodings or [\"utf-8\", \"utf-8-sig\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n\n    for encoding in encodings:\n        try:\n            dataframe = pd.read_csv(filepath, encoding=encoding, **kwargs)\n            print(f\"[jcds] Loaded CSV with encoding: {encoding}\")\n            return dataframe\n        except UnicodeDecodeError:\n            print(f\"[jcds] Failed to load with encoding: {encoding}\")\n            continue\n        except Exception as e:\n            print(f\"[jcds] Unexpected error with encoding '{encoding}': {e}\")\n            continue\n\n    print(f\"[jcds] \u274c Could not decode file: {filepath}\")\n    print(f\"[jcds] Preview of raw bytes:\")\n\n    try:\n        with open(filepath, \"rb\") as f:\n            raw = f.read(preview_bytes)\n            print(raw.decode(\"latin1\", errors=\"replace\"))  # fallback preview\n    except Exception as e:\n        print(f\"[jcds] Also failed to read raw bytes: {e}\")\n\n    raise ValueError(\n        f\"[jcds] Failed to decode file {filepath} with encodings {encodings}. \"\n        \"See preview above for troubleshooting.\"\n    )\n</code></pre>"},{"location":"api/#jcds.dataio.load_parquet","title":"<code>load_parquet(filepath: Union[str, Path], **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Load a Parquet file into a pandas DataFrame.</p>"},{"location":"api/#jcds.dataio.load_parquet--parameters","title":"Parameters","text":"<p>filepath : str or Path     Path to the Parquet file. **kwargs     Additional keyword arguments passed to <code>pd.read_parquet</code>.</p>"},{"location":"api/#jcds.dataio.load_parquet--returns","title":"Returns","text":"<p>pd.DataFrame     The loaded DataFrame.</p>"},{"location":"api/#jcds.dataio.load_parquet--notes","title":"Notes","text":"<p>Requires either <code>pyarrow</code> or <code>fastparquet</code> to be installed.</p> Source code in <code>src/jcds/dataio/io_utils.py</code> <pre><code>def load_parquet(filepath: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Load a Parquet file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    filepath : str or Path\n        Path to the Parquet file.\n    **kwargs\n        Additional keyword arguments passed to `pd.read_parquet`.\n\n    Returns\n    -------\n    pd.DataFrame\n        The loaded DataFrame.\n\n    Notes\n    -----\n    Requires either `pyarrow` or `fastparquet` to be installed.\n\n    \"\"\"\n    filepath = Path(filepath)\n    return pd.read_parquet(filepath, **kwargs)\n</code></pre>"},{"location":"api/#jcds.dataio.read_s3","title":"<code>read_s3(bucket_name: str, file_name: str, file_type: str = 'csv') -&gt; pd.DataFrame</code>","text":"<p>Download a public file from an S3 bucket and load it into a pandas DataFrame.</p>"},{"location":"api/#jcds.dataio.read_s3--parameters","title":"Parameters","text":"<p>bucket_name : str     The name of the public S3 bucket. file_name : str     The path to the file within the bucket. file_type : str, optional     The type of file to load. Supported values are 'csv' and 'excel'. Defaults to 'csv'.</p>"},{"location":"api/#jcds.dataio.read_s3--returns","title":"Returns","text":"<p>pd.DataFrame or None     The loaded DataFrame if successful, otherwise None.</p>"},{"location":"api/#jcds.dataio.read_s3--notes","title":"Notes","text":"<ul> <li>This function assumes the file is publicly accessible via a standard S3 URL.</li> <li>For Excel files, only <code>.xlsx</code> is supported.</li> <li>Requires the <code>requests</code>, <code>pandas</code>, and <code>io</code> modules.</li> <li>Prints an error message and returns None if the request fails or the file type is unsupported.</li> </ul> Source code in <code>src/jcds/dataio/s3_io.py</code> <pre><code>def read_s3(bucket_name: str, file_name: str, file_type: str = \"csv\") -&gt; pd.DataFrame:\n    \"\"\"Download a public file from an S3 bucket and load it into a pandas DataFrame.\n\n    Parameters\n    ----------\n    bucket_name : str\n        The name of the public S3 bucket.\n    file_name : str\n        The path to the file within the bucket.\n    file_type : str, optional\n        The type of file to load. Supported values are 'csv' and 'excel'. Defaults to 'csv'.\n\n    Returns\n    -------\n    pd.DataFrame or None\n        The loaded DataFrame if successful, otherwise None.\n\n    Notes\n    -----\n    - This function assumes the file is publicly accessible via a standard S3 URL.\n    - For Excel files, only `.xlsx` is supported.\n    - Requires the `requests`, `pandas`, and `io` modules.\n    - Prints an error message and returns None if the request fails or the file type is unsupported.\n\n    \"\"\"\n    url = f\"https://{bucket_name}.s3.amazonaws.com/{file_name}\"\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n\n        if file_type == \"csv\":\n            dataframe = pd.read_csv(BytesIO(response.content))\n        elif file_type == \"excel\":\n            dataframe = pd.read_excel(BytesIO(response.content))\n        else:\n            raise ValueError(\"Unsupported file type. Use 'csv' or 'excel'.\")\n        return dataframe\n\n    except ValueError:\n        raise  # Let unsupported type errors propagate for testing\n\n    except Exception as e:\n        print(f\"[jcds] Error loading file from S3: {e}\")\n        return None\n</code></pre>"},{"location":"api/#jcds.dataio.save_csv","title":"<code>save_csv(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None</code>","text":"<p>Save a DataFrame to a CSV file.</p>"},{"location":"api/#jcds.dataio.save_csv--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to save. filepath : str or Path     Destination file path. **kwargs     Additional keyword arguments passed to <code>DataFrame.to_csv</code>.</p>"},{"location":"api/#jcds.dataio.save_csv--notes","title":"Notes","text":"<p>Automatically creates the parent directory if it doesn't exist. Uses <code>index=False</code> by default unless overridden.</p> Source code in <code>src/jcds/dataio/io_utils.py</code> <pre><code>def save_csv(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None:\n    \"\"\"Save a DataFrame to a CSV file.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to save.\n    filepath : str or Path\n        Destination file path.\n    **kwargs\n        Additional keyword arguments passed to `DataFrame.to_csv`.\n\n    Notes\n    -----\n    Automatically creates the parent directory if it doesn't exist.\n    Uses `index=False` by default unless overridden.\n\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    # Default to index=False if not explicitly passed\n    if \"index\" not in kwargs:\n        kwargs[\"index\"] = False\n\n    dataframe.to_csv(filepath, **kwargs)\n</code></pre>"},{"location":"api/#jcds.dataio.save_parquet","title":"<code>save_parquet(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None</code>","text":"<p>Save a DataFrame to a Parquet file.</p>"},{"location":"api/#jcds.dataio.save_parquet--parameters","title":"Parameters","text":"<p>dataframe : pd.DataFrame     The DataFrame to save. filepath : str or Path     Destination file path. **kwargs     Additional keyword arguments passed to <code>DataFrame.to_parquet</code>.</p>"},{"location":"api/#jcds.dataio.save_parquet--notes","title":"Notes","text":"<p>Automatically creates the parent directory if it doesn't exist.</p> Source code in <code>src/jcds/dataio/io_utils.py</code> <pre><code>def save_parquet(dataframe: pd.DataFrame, filepath: Union[str, Path], **kwargs) -&gt; None:\n    \"\"\"Save a DataFrame to a Parquet file.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The DataFrame to save.\n    filepath : str or Path\n        Destination file path.\n    **kwargs\n        Additional keyword arguments passed to `DataFrame.to_parquet`.\n\n    Notes\n    -----\n    Automatically creates the parent directory if it doesn't exist.\n\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_parquet(filepath, index=False, **kwargs)\n</code></pre>"},{"location":"api/#reports","title":"Reports","text":""},{"location":"api/#jcds.reports.catvar_report","title":"<code>catvar_report(dataframe, columns=None)</code>","text":"<p>Display a summary report for categorical variables in a DataFrame.</p>"},{"location":"api/#jcds.reports.catvar_report--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The DataFrame to analyze. columns : str or list of str or None, optional     Name of a single categorical column (str), a list of column names (list of str),     or None to report on all detected categorical columns. Default is None.</p>"},{"location":"api/#jcds.reports.catvar_report--returns","title":"Returns","text":"<p>pandas.DataFrame or None     - If no valid categorical columns are selected, returns an empty DataFrame.     - Otherwise, prints a report for each column and returns None.</p>"},{"location":"api/#jcds.reports.catvar_report--notes","title":"Notes","text":"<ul> <li>Detects categorical columns via <code>show_catvar(dataframe)</code>.</li> <li>Computes missing-value stats with <code>show_missing_summary(dataframe, sort=False, threshold=0.0)</code>.</li> <li>Computes unique counts and top two modes via <code>count_unique_values(dataframe, col)</code>.</li> <li>The report includes, for each column:     \u2022 Non-missing count and missing count (%).     \u2022 Cardinality (number of unique values).     \u2022 Top two modes with their frequencies and percentage of non-missing.</li> <li>Docstring generated with assistance from ChatGPT</li> </ul> Source code in <code>src/jcds/reports/reports.py</code> <pre><code>def catvar_report(dataframe, columns=None):\n    \"\"\"\n    Display a summary report for categorical variables in a DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The DataFrame to analyze.\n    columns : str or list of str or None, optional\n        Name of a single categorical column (str), a list of column names (list of str),\n        or None to report on all detected categorical columns. Default is None.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        - If no valid categorical columns are selected, returns an empty DataFrame.\n        - Otherwise, prints a report for each column and returns None.\n\n    Notes\n    -----\n    - Detects categorical columns via `show_catvar(dataframe)`.\n    - Computes missing-value stats with `show_missing_summary(dataframe, sort=False, threshold=0.0)`.\n    - Computes unique counts and top two modes via `count_unique_values(dataframe, col)`.\n    - The report includes, for each column:\n        \u2022 Non-missing count and missing count (%).\n        \u2022 Cardinality (number of unique values).\n        \u2022 Top two modes with their frequencies and percentage of non-missing.\n    - Docstring generated with assistance from ChatGPT\n    \"\"\"\n    categorical_columns = show_catvar(dataframe)\n    columns_missing_values = show_missing_summary(dataframe, sort=False, threshold=0.0)\n\n    if isinstance(columns, str):\n        cols = [columns]\n    elif isinstance(columns, list):\n        cols = columns\n    else:\n        cols = categorical_columns\n\n    # validate columns\n    valid_columns = []\n    for c in cols:\n        if c in categorical_columns:\n            valid_columns.append(c)\n    if not valid_columns:\n        print(\"No valid categorical columns selected.\")\n        return pd.DataFrame()\n\n    total_rows = len(dataframe)\n\n    for col in valid_columns:\n\n        missing_count, pct_missing = columns_missing_values.get(col, (0, 0.0))\n        # Calculate non_missing values\n        non_missing = total_rows - missing_count\n        unique_values = count_unique_values(dataframe, col)\n        unique_count = unique_values[col][\"unique_count\"]\n        mode1 = unique_values[col][\"top_modes\"][0]\n        freq1 = mode1[1]\n        pct_freq1 = round(mode1[1] / non_missing * 100, 1)\n        mode2 = unique_values[col][\"top_modes\"][1]\n        freq2 = mode2[1]\n        pct_freq2 = round(mode2[1] / non_missing * 100, 1)\n\n        print(f\"\\nFeature: '{col}'\")\n        print(f\"===================================================================\")\n        print(\n            f\"Total: {non_missing}\\t\\t\\tMissing: {missing_count} ({pct_missing*100}%)\\t\\t\\tCardinality (Unique Values): {unique_count}\"\n        )\n        print(f\"Mode 1: {mode1[0]} \\t\\tFrequency: {freq1} ({pct_freq1}%)\")\n        print(f\"Mode 2: {mode2[0]} \\t\\tFequency: {freq2} ({pct_freq2}%)\")\n\n    print(f\"\\nTotal rows: {total_rows}\")\n</code></pre>"},{"location":"api/#jcds.reports.data_cardinality","title":"<code>data_cardinality(dataframe, show_columns=False)</code>","text":"<p>Summarizes the cardinality of the columns in the dataset.</p>"},{"location":"api/#jcds.reports.data_cardinality--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The input dataset.</p> bool, optional <p>Whether to display the list of columns in each category.</p>"},{"location":"api/#jcds.reports.data_cardinality--returns","title":"Returns","text":"<p>None     Prints summary information to the console.</p> Source code in <code>src/jcds/reports/reports.py</code> <pre><code>def data_cardinality(dataframe, show_columns=False):\n    \"\"\"\n    Summarizes the cardinality of the columns in the dataset.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The input dataset.\n\n    show_columns : bool, optional\n        Whether to display the list of columns in each category.\n\n    Returns\n    -------\n    None\n        Prints summary information to the console.\n    \"\"\"\n    # Threshold constants\n    NEAR_CONST_THRESHOLD = 0.95\n    LOW_CARD_MAX_UNIQUE = 10\n    HIGH_CARD_PERCENT_UNIQUE = 90\n\n    print(\"CARDINALITY REPORT\")\n\n    shape = show_shape(dataframe)\n    print(f\"\\nTotal columns analyzed: {shape[1]}\")\n\n    print(\"\\n[BINARY COLUMNS]\")\n    binary_list = show_binary_list(dataframe)\n    for key, value in binary_list.items():\n        total = len(value)\n        print(f\"There are {total} {key.replace('_', ' ')}.\")\n        if show_columns:\n            if total &gt; 0:\n                print(f\" * Columns: {value}\")\n\n    print(\"\\n[CONSTANT/NEAR CONSTANT COLUMNS]\")\n    const_var = show_constantvars(dataframe)\n    print(f\"There are {len(const_var)} constant columns.\")\n    if show_columns:\n        if len(const_var) &gt; 0:\n            print(f\" * Columns: {const_var}\")\n\n    near_constvar = show_nearconstvars(\n        dataframe, threshold=NEAR_CONST_THRESHOLD, verbose=False\n    )\n    print(\n        f\"There are {len(near_constvar)} near-constant columns with &gt;= {NEAR_CONST_THRESHOLD * 100:.0f}% of values being the same.\"\n    )\n    if show_columns:\n        if len(near_constvar) &gt; 0:\n            print(f\" * Columns: {near_constvar}\")\n\n    print(\"\\n[LOW CARDINALITY CATEGORICAL COLUMNS]\")\n    lowcardvars = show_lowcardvars(\n        dataframe, max_unique=LOW_CARD_MAX_UNIQUE, verbose=False\n    )\n    print(\n        f\" * There are {len(lowcardvars)} low cardinality columns with &lt;= {LOW_CARD_MAX_UNIQUE} unique values.\"\n    )\n    if show_columns:\n        print(\"Columns:\")\n        for col, n in lowcardvars:\n            print(f\" * {col}: {n} unique values\")\n\n    print(\"\\n[HIGH CARDINALITY CATEGORICAL COLUMNS]\")\n    highcardvars = show_highcardvars(\n        dataframe, percent_unique=HIGH_CARD_PERCENT_UNIQUE, verbose=False\n    )\n    print(\n        f\" * There are {len(highcardvars)} high cardinality variables with &gt;={HIGH_CARD_PERCENT_UNIQUE}% unique values.\"\n    )\n    if show_columns:\n        if len(highcardvars) &gt; 0:\n            print(f\" * Columns: {highcardvars}\")\n</code></pre>"},{"location":"api/#jcds.reports.data_info","title":"<code>data_info(dataframe, show_columns=False)</code>","text":"<p>Summarize the dataset's shape, memory usage, duplicates, and variable types.</p>"},{"location":"api/#jcds.reports.data_info--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The input dataset.</p> bool, optional <p>Whether to display the list of columns in each category.</p>"},{"location":"api/#jcds.reports.data_info--returns","title":"Returns","text":"<p>None     Prints summary information to the console.</p> Source code in <code>src/jcds/reports/reports.py</code> <pre><code>def data_info(dataframe, show_columns=False):\n    \"\"\"\n    Summarize the dataset's shape, memory usage, duplicates, and variable types.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The input dataset.\n\n    show_columns : bool, optional\n        Whether to display the list of columns in each category.\n\n    Returns\n    -------\n    None\n        Prints summary information to the console.\n    \"\"\"\n    # Threshold constants\n    ID_LIKE_COLS_THRESHOLD = 0.95\n\n    print(\"\\nSHAPE:\")\n    memory_use = show_memory_use(dataframe)\n    shape = show_shape(dataframe)\n    print(f\"There are {shape[0]} rows and {shape[1]} columns ({memory_use:.2f} MB).\")\n\n    print(\"\\nDUPLICATES:\")\n    dupes = show_dupes(dataframe)\n    print(f\"There are {dupes} duplicated rows.\")\n\n    print(\"\\nCOLUMNS/VARIABLES:\")\n\n    print(\"Column dType Summary:\")\n    dtype_summary = get_dtype_summary(dataframe)\n    for key, value in dtype_summary.items():\n        if value &gt; 0:\n            print(f\" * {key}: {value}\")\n\n    convar = show_convar(dataframe)\n    print(f\"There are {len(convar)} numerical (int/float/bool) variables.\")\n    if show_columns:\n        print(f\" * Columns: {convar}\")\n\n    catvar = show_catvar(dataframe)\n    print(f\"There are {len(catvar)} categorical (nominal/ordinal) variables.\")\n    if show_columns:\n        print(f\" * Columns: {catvar}\")\n\n    print(\"\\nDATETIME COLUMNS:\")\n    dt_cols = show_datetime_columns(dataframe)\n    possible_dt_cols = show_possible_datetime_columns(dataframe)\n    print(\n        f\"There are {len(dt_cols)} datetime variables and {len(possible_dt_cols)} possible datetime variables.\"\n    )\n\n    print(\"\\nOTHER COLUMN/VARIABLE INFO:\")\n    id_like_columns = count_id_like_columns(dataframe, threshold=ID_LIKE_COLS_THRESHOLD)\n    print(\n        f\"ID Like Columns (threshold = {ID_LIKE_COLS_THRESHOLD * 100}%): {id_like_columns}\"\n    )\n\n    mixed_columns = show_mixed_type_columns(dataframe)\n    print(f\"Columns with mixed datatypes: {len(mixed_columns)}\")\n    if show_columns:\n        print(f\" * Columns: {mixed_columns}\")\n</code></pre>"},{"location":"api/#jcds.reports.data_quality","title":"<code>data_quality(dataframe, show_columns=False)</code>","text":"<p>Print a comprehensive data quality report for the given DataFrame.</p> <p>This function summarizes key structural and content-based diagnostics to assess the cleanliness, consistency, and usability of a dataset. It includes checks for:</p> <ul> <li>Overall shape and memory usage</li> <li>Missing values (total, per row, and per column)</li> <li>Duplicate rows</li> <li>Constant and near-constant columns</li> <li>Mixed data types within columns</li> <li>High-cardinality categorical columns</li> </ul>"},{"location":"api/#jcds.reports.data_quality--parameters","title":"Parameters","text":"<p>dataframe : pandas.DataFrame     The input dataset to evaluate.</p> bool, optional (default=False) <p>If True, prints the list of columns associated with each quality issue (e.g. columns with missing values, constants).</p>"},{"location":"api/#jcds.reports.data_quality--returns","title":"Returns","text":"<p>None     Outputs the report directly to the console.</p> Source code in <code>src/jcds/reports/reports.py</code> <pre><code>def data_quality(dataframe, show_columns=False):\n    \"\"\"\n    Print a comprehensive data quality report for the given DataFrame.\n\n    This function summarizes key structural and content-based diagnostics to assess\n    the cleanliness, consistency, and usability of a dataset. It includes checks for:\n\n    - Overall shape and memory usage\n    - Missing values (total, per row, and per column)\n    - Duplicate rows\n    - Constant and near-constant columns\n    - Mixed data types within columns\n    - High-cardinality categorical columns\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        The input dataset to evaluate.\n\n    show_columns : bool, optional (default=False)\n        If True, prints the list of columns associated with each quality issue (e.g. columns with missing values, constants).\n\n    Returns\n    -------\n    None\n        Outputs the report directly to the console.\n    \"\"\"\n    NEAR_CONSTANT_COLUMNS_THRESHOLD = 0.95\n    HIGH_CARDINALITY_PERCENT = 60\n    print(\"DATA QUALITY REPORT\")\n    print(\"====================\")\n\n    # shape\n    rows, cols, dataframe_size, memory_usage = show_dimensions(dataframe)\n    print(f\"\\n * Total entries (rows * cols): {dataframe_size}\")\n    print(f\" * Memory usage: {memory_usage} MB\")\n    print(f\" * Rows: {rows}\")\n    print(f\" * Columns: {cols}\")\n\n    # missing data summary\n    print(\"\\nMISSING DATA:\")\n\n    total_missing = count_total_na(dataframe)\n    print(\n        f\" * Total entries: {total_missing} missing ({(total_missing / dataframe_size) * 100:.1f}%)\"\n    )\n\n    # missing rows\n    print(\"\\nROWS:\")\n    print(\"----------\")\n    rows_missing_any = count_rows_with_any_na(dataframe)\n    rows_missing_all = count_rows_with_all_na(dataframe)\n    print(f\" * Rows missing any: {rows_missing_any}\")\n    print(f\" * Rows missing all: {rows_missing_all}\")\n\n    # duplicate rows\n    duplicates = show_dupes(dataframe)\n    print(f\"\\nDUPLICATES: {duplicates}\")\n\n    # missing columns\n    print(\"\\nCOLUMNS:\")\n    print(\"----------------\")\n    missing_summary = show_missing_summary(dataframe, sort=True, threshold=0.0)\n    key_list = list(missing_summary.keys())\n    print(f\"Columns missing any: {len(missing_summary)}\")\n    if show_columns and missing_summary:\n        for key, value in missing_summary.items():\n            print(f\"\\t'{key}': {value[0]} missing ({value[1]:.1f}%)\")\n        print(f\"Column list: {key_list}\")\n\n    # constant columns\n    constant_cols = show_constantvars(dataframe)\n    print(f\"\\nCONSTANT: {len(constant_cols)}\")\n    if show_columns and constant_cols:\n        print(f\"Column list: {constant_cols}\")\n\n    # near constant columns\n    near_constant_columns = show_nearconstvars(\n        dataframe, threshold=NEAR_CONSTANT_COLUMNS_THRESHOLD, verbose=False\n    )\n    print(f\"\\nNEAR CONSTANT: {len(near_constant_columns)}\")\n    print(f\"\\t({NEAR_CONSTANT_COLUMNS_THRESHOLD * 100:.0f}% of values are the same)\")\n    if show_columns and near_constant_columns:\n        print(f\"\\tColumn list: {near_constant_columns}\")\n\n    # mixed data types\n    mixed_data_columns = show_mixed_type_columns(dataframe)\n    print(f\"\\nMIXED DATATYPES: {len(mixed_data_columns)}\")\n    if show_columns and mixed_data_columns:\n        print(f\"\\tColumn list: {mixed_data_columns}\")\n\n    # high cardinality\n    high_card_columns = show_highcardvars(\n        dataframe, percent_unique=HIGH_CARDINALITY_PERCENT, verbose=False\n    )\n    print(f\"\\nHIGH CARDINALITY: {len(high_card_columns)}\")\n    print(f\"\\t({HIGH_CARDINALITY_PERCENT}% &gt;= unique values)\")\n    if show_columns and high_card_columns:\n        for col in high_card_columns:\n            print(f\"\\t* '{col[0]}': {col[1]:.1f}%\")\n</code></pre>"},{"location":"api/#aws","title":"AWS","text":""},{"location":"api/#jcds.aws.list_s3_bucket","title":"<code>list_s3_bucket(bucket_name)</code>","text":"<p>List the contents (object keys) of a public Amazon S3 bucket using anonymous access.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--parameters","title":"Parameters","text":"<p>bucket_name : str     The name of the public S3 bucket to list contents from.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--returns","title":"Returns","text":"<p>None     Prints the keys (filenames) of the objects found in the bucket.</p>"},{"location":"api/#jcds.aws.list_s3_bucket--notes","title":"Notes","text":"<ul> <li>This function uses anonymous (unsigned) access and works only with public buckets.</li> <li>Requires the <code>boto3</code> and <code>botocore</code> libraries.</li> <li>If the bucket is not public or an error occurs, an error message is printed.</li> </ul> Source code in <code>src/jcds/aws/s3_utils.py</code> <pre><code>def list_s3_bucket(bucket_name):\n    \"\"\"\n    List the contents (object keys) of a public Amazon S3 bucket using anonymous access.\n\n    Parameters\n    ----------\n    bucket_name : str\n        The name of the public S3 bucket to list contents from.\n\n    Returns\n    -------\n    None\n        Prints the keys (filenames) of the objects found in the bucket.\n\n    Notes\n    -----\n    - This function uses anonymous (unsigned) access and works only with public buckets.\n    - Requires the `boto3` and `botocore` libraries.\n    - If the bucket is not public or an error occurs, an error message is printed.\n\n    \"\"\"\n\n    try:\n        import boto3\n        from botocore import UNSIGNED\n        from botocore.config import Config\n    except ImportError as e:\n        print(\"Required library is not installed:\", e)\n        return\n\n    # create anonymous s3 client\n    s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n\n    try:\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        for obj in response.get(\"Contents\", []):\n            print(obj[\"Key\"])\n    except Exception as e:\n        print(\"Error accessing bucket:\", e)\n</code></pre>"},{"location":"api/#jcds-package-overview","title":"jcds Package Overview","text":"<p>options: members: - help</p>"},{"location":"api/#jcds.help","title":"<code>help(func_name=None, namespace=None)</code>","text":"<p>Unified help system for the jcds package.</p>"},{"location":"api/#jcds.help--parameters","title":"Parameters","text":"<p>func_name : str or None     If provided, shows the docstring for the named function.     If None, lists all available public functions. namespace : dict or None     Internal override used to pass a custom namespace.     If None, collects all callable functions from jcds submodules.</p> Source code in <code>src/jcds/__init__.py</code> <pre><code>def help(func_name=None, namespace=None):\n    \"\"\"\n    Unified help system for the jcds package.\n\n    Parameters\n    ----------\n    func_name : str or None\n        If provided, shows the docstring for the named function.\n        If None, lists all available public functions.\n    namespace : dict or None\n        Internal override used to pass a custom namespace.\n        If None, collects all callable functions from jcds submodules.\n    \"\"\"\n    import inspect as pyinspect\n    from jcds import eda, aws, dataio\n\n    if namespace is None:\n        namespace = {}\n\n        # Dynamically collect public callables from submodules\n        for mod in [eda, aws, dataio]:\n            for name in dir(mod):\n                # Skip all dunder names like __file__, __name__, etc.\n                if name.startswith(\"__\") and name.endswith(\"__\"):\n                    continue\n                obj = getattr(mod, name)\n                if callable(obj):\n                    namespace[name] = obj\n\n    functions = {name: obj for name, obj in namespace.items() if name != \"help\"}\n\n    if func_name is None:\n        print(\"Available functions in jcds:\\n\")\n        for name in sorted(functions):\n            print(f\"  - {name}\")\n        print('\\nUse jcds.help(\"function_name\") to see its documentation.')\n    else:\n        func = functions.get(func_name)\n        if func:\n            print(f\"\\nHelp for '{func_name}':\\n\")\n            print(pyinspect.getdoc(func) or \"(No docstring provided)\")\n        else:\n            print(f\"Function '{func_name}' not found.\")\n</code></pre>"}]}